[{"title":"ELK","date":"2017-08-14T06:16:28.000Z","path":"2017/08/14/ELK/","text":"案例背景介绍本来想学用一下flume， 把多台服务器的日志做统一处理，查阅资料后发现ELK更适合就尝试着搭建了ELK服务， 其实是ELKF（Elasticsearch logstash kibana Filebeat） 日志来源于两台服务器上的2个（4个： 2*2）应用； 日志采用log4j格式打印，目前设置日志级别为INFO，因此日志量比较大。 解决的问题： 登录多个服务器上查找日志 环境&amp;部署filebeat分别部署在2台应用服务器上，logstash单独部署（grok耗费资源），es和kibana部署在一台服务器（es作为存储 需提前分配存储空间） Filebeat介绍使用filebeat是因为 主要担心logstash与应用服务器 抢占资源。filebeat支持直接输出到elasticsearch（日志文件为json格式可以直接输出到es），也可以输出到logstash（本案例使用grok解析日志格式），也支持Redis 和 RocketMQ 安装 -linux x86_641wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.0.0-beta1-linux-x86_64.tar.gz 官网下载 其他版本 配置编辑 filebeat.yml 直接按照注释的说明就可以配置文件路径支持通配符123456# 日志文件路径-path /opt/../../../server*/System*.log# elasticsearch 配置# logstatsh 配置 运行12./filebeat -e -c filebeat.ymlnohup ./filebeat -e -c filebeat.yml &gt;/dev/null 2&gt;&amp;1 &amp; logstash介绍logstash 可以读流文件（socket读取），也可以多个logstash agent输出到logstash汇总应用本案例主要使用logstash解决两个问题： grok解析日志 生成json格式 多行输入的解析处理 安装1wget https://artifacts.elastic.co/downloads/logstash/logstash-5.5.1.tar.gz 依赖： jdk 1.8 配置 config文件config文件主要定义用户的处理规则注意： conf目录下的配置文件是logstash的基础配置文件，主要包括jvm配置和服务配置主要的工作量： 配置grok表达式以解析应用日志。 参考官网的配置示例最终的配置如下：12 grok说明说明： 自定义的pattern 可自定义存储，比如：patterns/extra, grok引入即可使用123456789# contents of ./patterns/postfix:POSTFIX_QUEUEID [0-9A-F]&#123;10,11&#125;# filter &#123; grok &#123; patterns_dir =&gt; [\"./patterns\"] match =&gt; &#123; \"message\" =&gt; \"%&#123;SYSLOGBASE&#125; %&#123;POSTFIX_QUEUEID:queue_id&#125;: %&#123;GREEDYDATA:syslog_message&#125;\" &#125; &#125;&#125; 正则说明\\s：匹配任何不可见字符，包括空格、制表符、换页符等等。等价于[ \\f\\n\\r\\t\\v]。+表示匹配次数为1次或者多次 (?:pattern)非获取匹配，匹配pattern但不获取匹配结果，不进行存储供以后使用。这在使用或字符“(|)”来组合一个模式的各个部分是很有用。例如“industr(?:y|ies)”就是一个比“industry|industries”更简略的表达式。(?=pattern)非获取匹配，正向肯定预查，在任何匹配pattern的字符串开始处匹配查找字符串，该匹配不需要获取供以后使用。例如，“Windows(?=95|98|NT|2000)”能匹配“Windows2000”中的“Windows”，但不能匹配“Windows3.1”中的“Windows”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。(?!pattern)非获取匹配，正向否定预查，在任何不匹配pattern的字符串开始处匹配查找字符串，该匹配不需要获取供以后使用。例如“Windows(?!95|98|NT|2000)”能匹配“Windows3.1”中的“Windows”，但不能匹配“Windows2000”中的“Windows”。(?&lt;=pattern)非获取匹配，反向肯定预查，与正向肯定预查类似，只是方向相反。例如，“(?&lt;=95|98|NT|2000)Windows”能匹配“2000Windows”中的“Windows”，但不能匹配“3.1Windows”中的“Windows”。(?&lt;!pattern)非获取匹配，反向否定预查，与正向否定预查类似，只是方向相反。例如“(?&lt;!95|98|NT|2000)Windows”能匹配“3.1Windows”中的“Windows”，但不能匹配“2000Windows”中的“Windows”。这个地方不正确，有问题 .\\d+：表示点后面跟一个或者多个 数字，(?:.\\d+)?表示点后面跟一个或多个数字这种情况出现0次或者多次，如果为0次，则request_time为一个整数。所以匹配到的结果可能为123.456或者123或者123.4.5.6，这些都满足条件 分组语法 捕获(exp) 匹配exp,并捕获文本到自动命名的组里(?exp) 匹配exp,并捕获文本到名称为name的组里，也可以写成(?’name’exp)(?:exp) 匹配exp,不捕获匹配的文本位置指定(?=exp) 匹配exp前面的位置(?&lt;=exp) 匹配exp后面的位置(?!exp) 匹配后面跟的不是exp的位置(?&lt;!exp) 匹配前面不是exp的位置注释(?#comment) 这种类型的组不对正则表达式的处理产生任何影响，只是为了提供让人阅读注释 grok使用测试网站：grokdebug.herokuapp.com参考示例 运行123bin/logstash -f logstash.conf [-path conf]nohup bin/logstash -f etc/ &amp; 详细的命令行参数说明 ES介绍安装1wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.1.tar.gz 依赖 jdk1.8 运行12bin/elasticsearchbin/elasticsearch -d #后台运行 说明： 不能使用root 启动，为es单独建用户 vim /etc/sysctl.confvm.max_map_count=655360 vim /etc/security/limits.conf hard nofile 65536 soft nofile 65536 es使用curl –silent ‘http://127.0.0.1:9200/_cat/indices‘ | cut -d\\ -f2 /_aliases?pretty=1 /_stats/_stats/{metric}/_stats/{metric}/{indexMetric}/{index}/_stats/{index}/_stats/{metric} Elasticsearch关键概念根据官网自己的介绍，Elasticsearch是一个分布式搜索服务，提供Restful API，底层基于Lucene，采用多shard的方式保证数据安全，并且提供自动resharding的功能，加之github等大型的站点也采用Elasticsearch作为其搜索服务，我们决定在项目中使用Elasticsearch。 数据：Index：Elasticsearch用来存储数据的逻辑区域，它类似于关系型数据库中的db概念。一个index可以在一个或者多个shard上面，同时一个shard也可能会有多个replicas。Document：Elasticsearch里面存储的实体数据，类似于关系数据中一个table里面的一行数据。document由多个field组成，不同的document里面同名的field一定具有相同的类型。document里面field可以重复出现，也就是一个field会有多个值，即multivalued。Document type：为了查询需要，一个index可能会有多种document，也就是document type，但需要注意，不同document里面同名的field一定要是相同类型的。Mapping：存储field的相关映射信息，不同document type会有不同的mapping。对于熟悉MySQL的童鞋，我们只需要大概认为Index就是一个db，document就是一行数据，field就是table的column，mapping就是table的定义，而document type就是一个table就可以了。 服务层：Node: 一个server实例。Cluster：多个node组成cluster。Shard：数据分片，一个index可能会存在于多个shards，不同shards可能在不同nodes。Replica：shard的备份，有一个primary shard，其余的叫做replica shards。Elasticsearch之所以能动态resharding，主要在于它最开始就预先分配了多个shards（貌似是1024），然后以shard为单位进行数据迁移。这个做法其实在分布式领域非常的普遍，codis就是使用了1024个slot来进行数据迁移。 因为任意一个index都可配置多个replica，通过冗余备份的方式保证了数据的安全性，同时replica也能分担读压力，类似于MySQL中的slave Lucene关键概念Document：用来索引和搜索的主要数据源，包含一个或者多个Field，而这些Field则包含我们跟Lucene交互的数据。Field：Document的一个组成部分，有两个部分组成，name和value。Term：不可分割的单词，搜索最小单元。Token：一个Term呈现方式，包含这个Term的内容，在文档中的起始位置，以及类型。 作者：siddontang链接：http://www.jianshu.com/p/05cff717563c來源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 kibana介绍安装 linux x86_641wget https://artifacts.elastic.co/downloads/kibana/kibana-5.5.1-linux-x86_64.tar.gz 其他版本下载 配置 config/kibana.yml Set elasticsearch.url to point at your Elasticsearch instance 运行1nohup bin/kibana &amp; 使用索引设置filebeat-*日志正常解析为多个域后， kibana会自动识别field并作为查询条件 query附：log4j 输出格式%n - 换行 %m - 日志内容 %p - 日志级别(FATAL, ERROR, WARN, INFO, DEBUG or custom) %r - 程序启动到现在的毫秒数 %% - percent sign in output %t - 当前线程名 %d - 日期和时间, 常用的格式有 %d{DATE}, %d{ABSOLUTE}, %d{HH:mm:ss,SSS}, %d{ddMMyyyy HH:mm:ss,SSS}。。。 %l - 同 %F%L%C%M %F - java源文件名 %L - java源码行数 %C - java类名,%C{1} 输出最后一个元素 %M-java方法名 待补充： es集群配置 kibana es认证设置 es中文分词","categories":[],"tags":[]},{"title":"公交车上","date":"2017-08-12T14:12:42.000Z","path":"2017/08/12/公交车上/","text":"下雨的周末 路上车很少 我在公车上看一本书 书名叫 “坐公交车的人” 应该没什么名气 但是本很舒服的书 ， 窗外的雨有时大 有时小 偶尔几道闪电挑逗你的视线 前面的小朋友 背着一把大大的吉他 下雨的夏天 很舒服 ， 平日拥挤的城市 今天很冷清 车开的很快 我读的很慢 窗外，雨点在跳跃 ~ 书","categories":[{"name":"散文","slug":"散文","permalink":"http://www.sillystone.info/categories/散文/"}],"tags":[{"name":"散文","slug":"散文","permalink":"http://www.sillystone.info/tags/散文/"}]},{"title":"Excel","date":"2017-08-08T11:30:36.000Z","path":"2017/08/08/Excel/","text":"如何快速成为数据分析师函数技巧实战 函数清洗处理类主要是文本、格式以及脏数据的清洗和转换。很多数据并不是直接拿来就能用的，需要经过数据分析人员的清理。数据越多，这个步骤花费的时间越长。 Trim 清除掉字符串两边的空格。 MySQL有同名函数，Python有近似函数strip。 Concatenate =Concatenate(单元格1，单元格2……) 合并单元格中的内容，还有另一种合并方式是&amp; 。”我”&amp;”很”&amp;”帅” ＝ 我很帅。当需要合并的内容过多时，concatenate的效率快也优雅。 MySQL有近似函数concat。 Replace =Replace（指定字符串，哪个位置开始替换，替换几个字符，替换成什么） 替换掉单元格的字符串，清洗使用较多。 MySQL中有同名函数，Python中有同名函数。 Substitute 和replace接近，区别是替换为全局替换，没有起始位置的概念 Left／Right／Mid =Mid(指定字符串，开始位置，截取长度) 截取字符串中的字符。Left/Right（指定字符串，截取长度）。left为从左，right为从右，mid如上文示意。 MySQL中有同名函数。 Len／Lenb 返回字符串的长度，在len中，中文计算为一个，在lenb中，中文计算为两个。MySQL中有同名函数，Python中有同名函数。 Find =Find（要查找字符，指定字符串，第几个字符） 查找某字符串出现的位置，可以指定为第几次出现，与Left／Right／Mid结合能完成简单的文本提取MySQL中有近似函数 find_in_set，Python中有同名函数。 Search 和Find类似，区别是Search大小写不敏感，但支持＊通配符 Text 将数值转化为指定的文本格式，可以和时间序列函数一起看 关联匹配类在进行多表关联或者行列比对时用到的函数，越复杂的表用得越多。多说一句，良好的表习惯可以减少这类函数的使用。 Lookup =Lookup（查找的值，值所在的位置，返回相应位置的值） 最被忽略的函数，功能性和Vlookup一样，但是引申有数组匹配和二分法。 Vlookup =Vlookup(查找的值，哪里找，找哪个位置的值，是否精准匹配) Excel第一大难关，因为涉及的逻辑对新手较复杂，通俗的理解是查找到某个值然后黏贴过来。 Index ＝Index（查找的区域，区域内第几行，区域内第几列） 和Match组合，媲美Vlookup，但是功能更强大。 Match ＝Match（查找指定的值，查找所在区域，查找方式的参数） 和Lookup类似，但是可以按照指定方式查找，比如大于、小于或等于。返回值所在的位置。 Row 返回单元格所在的行 Column 返回单元格所在的列 Offset ＝Offset（指定点，偏移多少行，偏移多少列，返回多少行，返回多少列） 建立坐标系，以坐标系为原点，返回距离原点的值或者区域。正数代表向下或向右，负数则相反。 逻辑运算类数据分析中不得不用到逻辑运算，逻辑运算返回的均是布尔类型，True和False。很多复杂的数据分析会牵扯到较多的逻辑运算 IF 经典的如果但是，在后期的Python中，也会经常用到，当然会有许多更优雅的写法。也有ifs用法，取代if(and())的写法。 MySQL中有同名函数，Python中有同名函数。 And 全部参数为True，则返回True，经常用于多条件判断。 MySQL中有同名函数，Python中有同名函数。 Or 只要参数有一个True，则返回Ture，经常用于多条件判断。 MySQL中有同名函数，Python中有同名函数。 IS系列 常用判断检验，返回的都是布尔数值True和False。常用ISERR，ISERROR，ISNA，ISTEXT，可以和IF嵌套使用。 计算统计类常用的基础计算、分析、统计函数，以描述性统计为准。具体含义在后续的统计章节再展开。 Sum／Sumif／Sumifs 统计满足条件的单元格总和，SQL有中同名函数。 MySQL中有同名函数，Python中有同名函数。 Sumproduct 统计总和相关，如果有两列数据销量和单价，现在要求卖出增加，用sumproduct是最方便的。 MySQL中有同名函数。 Count／Countif／Countifs 统计满足条件的字符串个数 MySQL中有同名函数，Python中有同名函数。 Max 返回数组或引用区域的最大值 MySQL中有同名函数，Python中有同名函数。 Min 返回数组或引用区域的最小值 MySQL中有同名函数，Python中有同名函数。 Rank 排序，返回指定值在引用区域的排名，重复值同一排名。 SQL中有近似函数row_number() 。 Rand／Randbetween 常用随机抽样，前者返回0~1之间的随机值，后者可以指定范围。 MySQL中有同名函数。 Averagea 求平均值，也有Averageaif，Averageaifs MySQL中有同名函数，python有近似函数mean。 Quartile =Quartile（指定区域，分位参数） 计算四分位数，比如1~100的数字中，25分位就是按从小到大排列，在25%位置的数字，即25。参数0代表最小值，参数4代表最大值，1~3对应25、50（中位数）、75分位 Stdev 求标准差，统计型函数，后续数据分析再讲到 Substotal =Substotal（引用区域，参数） 汇总型函数，将平均值、计数、最大最小、相乘、标准差、求和、方差等参数化，换言之，只要会了这个函数，上面的都可以抛弃掉了。 Int／Round 取整函数，int向下取整，round按小数位取数。 round(3.1415,2) =3.14 ; round(3.1415,1)=3.1 时间序列类专门用于处理时间格式以及转换，时间序列在金融、财务等数据分析中占有较大比重。时机序列的处理函数比我列举了还要复杂，比如时区、分片、复杂计算等。这里只做一个简单概述。 Year 返回日期中的年 MySQL中有同名函数。 Month 返回日期中的月 MySQL中有同名函数。 Weekday =Weekday(指定时间，参数) 返回指定时间为一周中的第几天，参数为1代表从星期日开始算作第一天，参数为2代表从星期一开始算作第一天（中西方差异）。我们中国用2为参数即可。 MySQL中有同名函数。 Weeknum =Weeknum(指定时间，参数) 返回一年中的第几个星期，后面的参数类同weekday，意思是从周日算还是周一。 MySQL中有近似函数 week。 Day 返回日期中的日（第几号） MySQL中有同名函数。 Date =Date（年，月，日） 时间转换函数，等于将year()，month()，day()合并 MySQL中有近似函数 date_format。 Now 返回当前时间戳，动态函数 MySQL中有同名函数。 Today 返回今天的日期，动态函数 MySQL中有同名函数。 Datedif =Datedif（开始日期，结束日期，参数） 日期计算函数，计算两日期的差。参数决定返回的是年还是月等。 MySQL中有近似函数 DateDiff。","categories":[],"tags":[]},{"title":"db2 问题汇总","date":"2017-08-08T09:37:22.000Z","path":"2017/08/08/db2-问题汇总/","text":"创建数据库创建数据库失败， 报错： 解决:数据库名称不能和主机名相同， 修改主机名解决。 DATA CAPTURE数据库运行缓慢， 存在等待的内容是： ALTER TABLE t_table_name DATA CAPTURE NONE 以下是查询出来的 关于DATA CAPTURE解释：For updates, DB2 records undo/ redo records in the log. There’s an option on a CREATE or ALTER TABLE definition called DATA CAPTURE. The two settings are DATA CAPTURE CHANGES or DATA CAPTURE NONE, the default being NONE. For inserts and deletes, DB2 is always recording a full image of the row affected, but for updates, DB2 can minimize the data recorded in the log. The DATA CAPTURE CHANGES feature tells DB2 you’re planning to use the log information for some sort of data replication or log analysis. There are several data replication tools that can read log records or use a DB2 log exit to replicate database changes to other databases or processes. In order for the replication to work properly for updates, it needs the entire before and after image of the rows changed. So, DATA CAPTURE CHANGES will cause DB2 to log the entire before and after images of rows changed via update statements in the undo/redo records in the log. DB2 will minimize the information logged during updates if DATA CAPTURE NONE is set for a table. A good performance practice is to specify DATA CAPTURE NONE for a table unless you’re specifically planning on replicating the table via a product that reads log records.","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"db2","slug":"db2","permalink":"http://www.sillystone.info/tags/db2/"}]},{"title":"数据架构","date":"2017-08-05T05:25:10.000Z","path":"2017/08/05/数据架构/","text":"读完数据架构 摘要 封面 典型企业架构模型 词汇","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"数据架构","slug":"数据架构","permalink":"http://www.sillystone.info/tags/数据架构/"}]},{"title":"Das Leben der Anderen","date":"2017-08-04T16:35:38.000Z","path":"2017/08/05/Das-Leben-der-Anderen/","text":"Das Leben der Anderen– 79th Academy Awards 最佳外语片 从此喜欢上德国电影 HGW XX/7","categories":[{"name":"散文","slug":"散文","permalink":"http://www.sillystone.info/categories/散文/"}],"tags":[{"name":"日记","slug":"日记","permalink":"http://www.sillystone.info/tags/日记/"}]},{"title":"一只蚊子","date":"2017-08-02T12:30:21.000Z","path":"2017/08/02/一只蚊子/","text":"等公车的时候，猛地觉得手背有点疼， 发现一只黑色的蚊子在我手背上 我是那种手背血管凸起的体质， 它伏在一根凸起的血管上，享受着 我把手中的伞放下， 飞起一巴掌，留下一滩血迹 我的血，比我想象中要红，也更大 迅速擦掉，我忍不了血色 今天是个下雨天，闷热潮湿的桑拿天 蚊子活跃的天气 可能他们也不喜欢这么热 太热了就需要解渴的饮料 就得忙碌着找我这样的 不幸的是它遇到了我 其实我还算是个好人 一般被叮一两下也无所谓 忍几天就慢慢好了 也不是我心肠特好 主要是一般察觉不到被咬 这只蚊子，也是个没心眼的家伙 中了头彩遇到我 还找到了血管 开心的昏头了吧，吸这么大口 吸得我都觉得 被吸走三成功力 活该被拍死啊 雨莫名的下大了，车还不来。。。 它也真是不走运 为什么要吸那么大口呢，慢慢喝就好了 可能因为晚饭没吃饭吧，血糖估计很低 慢慢吸估计不过瘾 我就不喜欢慢慢吸，喝酸奶喜欢用勺 哎，可怜的家伙 公车终于来了， 已经快8点了，车上没几个人 有了空调舒服多了。 手背上慢慢肿了一点 不过还好，没有肿很高。 我对南方的蚊子过敏，手会肿的和馒头一样 它是北方的 可怜的家伙，死了都留不下一个包 哎，这个下雨的夜晚 突然想起了你 饭好了，我去吃了。","categories":[{"name":"散文","slug":"散文","permalink":"http://www.sillystone.info/categories/散文/"}],"tags":[{"name":"日记","slug":"日记","permalink":"http://www.sillystone.info/tags/日记/"}]},{"title":"AWS 技术峰会","date":"2017-07-26T14:59:59.000Z","path":"2017/07/26/AWS-技术峰会/","text":"AWS峰会，国家会议中心，逃班散心 AWS 产品基础服务 计算： EC2， EC2 container service，AWS Lambda 数据： Amazon Aurora， Amazon RDS，Amazon ElastiCache， Amazon Redshift 运维相关 存储： S3 网络： 管理工具 安全身份合规：AWS Shield 开发应用支撑 人工智能： Amazon lex, Amazon Rekognition, Amazon Polly Iot: 大数据（分析）：Amazon Athena Amazon ERM（hadoop） Kinesis Redshift NOTE 云服务： Amazon Microsoft google 云上的数据中心方案，混合云方案，本地数据&amp;云数据结合方案（关键：网络，安全） 基于云服务的开发测试部署方案 Iot 照片","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"AWS技术峰会","slug":"AWS技术峰会","permalink":"http://www.sillystone.info/tags/AWS技术峰会/"}]},{"title":"我的前半生","date":"2017-07-25T03:44:17.000Z","path":"2017/07/25/wo-de-qian-ban-sheng/","text":"讲的太透彻，活的太精明，能有多少快乐。","categories":[{"name":"随笔","slug":"随笔","permalink":"http://www.sillystone.info/categories/随笔/"}],"tags":[{"name":"日记","slug":"日记","permalink":"http://www.sillystone.info/tags/日记/"}]},{"title":"Freemarker cache","date":"2017-07-21T08:03:26.000Z","path":"2017/07/21/Freemarker-cache/","text":"模板缓存 说明FreeMarker 是会缓存模板的(假设使用 Configuration 对象的方法来创建 Template 对象)。这就是说当调用 getTemplate方法时，FreeMarker不但返回了 Template 对象，而且还会将它存储在缓存中， 当下一次再以相同(或相等)路径调用 getTemplate 方法时， 那么它只返回缓存的 Template 实例， 而不会再次加载和解析模板文件了。 如果更改了模板文件，当下次调用模板时，FreeMarker 将会自动重新载入和解析模板。 然而，要检查模板文件是否改变内容了是需要时间的，有一个 Configuration 级别的设置被称作”更新延迟”，它可以用来配置这个时间。 这个时间就是从上次对某个模板检查更新后，FreeMarker再次检查模板所要间隔的时间。 其默认值是5秒。如果想要看到模板立即更新的效果，那么就要把它设置为0。 要注意某些模板加载器也许在模板更新时可能会有问题。 例如，典型的例子就是在基于类加载器的模板加载器就不会注意到模板文件内容的改变。 当调用了 getTemplate 方法时， 与此同时FreeMarker意识到这个模板文件已经被移除了，所以这个模板也会从缓存中移除。 如果Java虚拟机认为会有内存溢出时，默认情况它会从缓存中移除任意模板。 此外，你还可以使用 Configuration 对象的 clearTemplateCache 方法手动清空缓存。 何时将一个被缓存了的模板清除的实际应用策略是由配置的属性 cache_storage 来确定的，通过这个属性可以配置任何 CacheStorage 的实现。对于大多数用户来说， 使用 freemarker.cache.MruCacheStorage 就足够了。 这个缓存存储实现了二级最近使用的缓存。在第一级缓存中， 组件都被强烈引用到特定的最大数目(引用次数最多的组件不会被Java虚拟机抛弃， 而引用次数很少的组件则相反)。当超过最大数量时， 最近最少使用的组件将被送至二级缓存中，在那里它们被很少引用， 直到达到另一个最大的数目。引用强度的大小可以由构造方法来指定。 例如，设置强烈部分为20，轻微部分为250： cfg.setCacheStorage(new freemarker.cache.MruCacheStorage(20, 250))或者，使用 MruCacheStorage 缓存， 它是默认的缓存存储实现： cfg.setSetting(Configuration.CACHE_STORAGE_KEY, “strong:20, soft:250”);当创建了一个新的 Configuration 对象时， 它使用一个 strongSizeLimit 值为0的 MruCacheStorage 缓存来初始化， softSizeLimit 的值是 Integer.MAX_VALUE (也就是在实际中，是无限大的)。但是使用非0的 strongSizeLimit 对于高负载的服务器来说也许是一个更好的策略，对于少量引用的组件来说， 如果资源消耗已经很高的话，Java虚拟机往往会引发更高的资源消耗， 因为它不断从缓存中抛出经常使用的模板，这些模板还不得不再次加载和解析 spring示例12345&lt;bean id=\"freemarkerMailConfiguration\" class=\"org.springframework.ui.freemarker.FreeMarkerConfigurationFactoryBean\"&gt; &lt;property name=\"templateLoaderPaths\" value=\"classpath:emailtemplates/task,classpath:emailtemplates/user\"/&gt; &lt;!-- Activate the following to disable template caching --&gt; &lt;property name=\"freemarkerSettings\" value=\"cache_storage=freemarker.cache.NullCacheStorage\" /&gt;&lt;/bean&gt; 自定义 freemarkerConfigure 123456&lt;bean id=\"freemarkerConfig\" class=\"com.xxxx.framework.web.EasyFreeMarkerConfigurer\"&gt; &lt;property name=\"freemarkerSettings\"&gt; &lt;props&gt; &lt;prop key=\"cache_storage\"&gt;freemarker.cache.NullCacheStorage&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; 1234567public class EasyFreeMarkerConfigurer extends FreeMarkerConfigurer &#123; // ... public void setfreemarkerSettings(Properties settings) &#123; // ... user defined super.setFreemarkerSettigs(settings); &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.sillystone.info/tags/java/"},{"name":"freemarker","slug":"freemarker","permalink":"http://www.sillystone.info/tags/freemarker/"}]},{"title":"google translate","date":"2017-07-14T01:07:23.000Z","path":"2017/07/14/google-translate/","text":"需求使用Google translate api 批量翻译文件 调研Google translate API官方文档 需要使用google云平台提供的服务 开通云平台（试用1年） 新建google translate 项目 生成key 使用限制： 每天 2,000,000 characters 100 seconds 100,000 characters 100 seconds 1000 request Google会提供每日统计结果。 javascript for google translate api使用github 上Localize 的google-translate 多语言文件处理(csv格式)使用github 上 fast-csv 开发问题一，javascript stream 处理readstream -&gt; pipe( processStream )processStream -&gt; pipe( writeStream) fast-csv 即 processStreamon(“data”, callProcessFunction(){//process;}) 问题二，nodejs 实现 for循环中 同步等待后执行原因： 单条循环调用，差不多500多笔 google开始返回报错 解决思路：使用 定时器，每批调用后 等待3秒， 需要同步等待尝试了 generator co最终： 使用递归调用 1234567891011121314151617181920212223242526272829303132333435363738394041424344let csvdata = []; // array for translatelet ITEM_SIZe = 20; // translate record number every time// for iterator sleep delay sencondsfunction forWithDelay(i, length, array, fn, delay) &#123; setTimeout(function () &#123; fn(i, length, array); i++; if (i * length &lt; array.length) &#123; forWithDelay(i, length, array, fn, delay); &#125; &#125;, delay);&#125;// function call google translate &amp; process data copy from array function callTranslate(i, length, array) &#123; let tranData = []; const arrayLength = array.length; for( let j = 0; j&lt;length; j++) &#123; if (i*length + j &lt; arrayLength)&#123; tranData[j] = array[i*length +j][1]; array[i*length +j][2] =\"AA\"; //console.log(array[i*length +j]); &#125; &#125; googleTranslate.translate(tranData, 'en', function(err, translation) &#123; if (err) &#123; console.log(err.stack()); return; &#125; if (!translation ) &#123; console.log(\"EEE\"); &#125; else &#123; translation.forEach(function(item,index,arr) &#123; console.log(index); console.log(item.translatedText); array[i*length+index][2] = item.translatedText; console.log(array[i*length+index]); csvWriteStream.write(array[i*length+index]); &#125;); &#125; &#125;);&#125;forWithDelay(0, ITEM_SIZE, csvdata, callTranslate, 3000);","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"http://www.sillystone.info/tags/javascript/"}]},{"title":"背影","date":"2017-07-06T15:07:10.000Z","path":"2017/07/06/背影/","text":"我怀念往昔 那纯真， 那失败， 那多情， 那冷漠~~~ 然而， 我总怀念， 不是那时光， 是那背影。","categories":[],"tags":[{"name":"诗","slug":"诗","permalink":"http://www.sillystone.info/tags/诗/"}]},{"title":"mariadb","date":"2017-07-04T07:23:59.000Z","path":"2017/07/04/mariadb/","text":"存储引擎存储引擎介绍 XtraDB is the best choice in the majority of cases. It is a performance-enhanced fork of InnoDB and is MariaDB&apos;s default engine until MariaDB 10.1. InnoDB is a good general transaction storage engine. It is the default MySQL storage engine, and default MariaDB 10.2 storage engine, but in earlier releases XtraDB is a performance enhanced fork of InnoDB, and is usually preferred. Aria, MariaDB&apos;s more modern improvement on MyISAM, has a small footprint and allows for easy copying between systems. MyISAM has a small footprint and allows for easy copying between systems. MyISAM is MySQL&apos;s oldest storage engine. There is usually little reason to use it except for legacy purposes. Aria is MariaDB&apos;s more modern improvement. InnODB引擎支持众多特性：a) 支持ACID，简单地说就是支持事务完整性、一致性；b) 支持行锁，以及类似ORACLE的一致性读，多用户并发；c) 独有的聚集索引主键设计方式，可大幅提升并发读写性能；d) 支持外键；e) 支持崩溃数据自修复；","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"mariadb","slug":"mariadb","permalink":"http://www.sillystone.info/tags/mariadb/"}]},{"title":"redis","date":"2017-07-04T06:13:42.000Z","path":"2017/07/04/redis-config/","text":"detail info clusterredis cluster 官方介绍特点： 节点间数据自动同步 节点间数据自动分配 故障隔离 所有节点彼此互联，使用二进制协议传输 客户端可以连接任意结点（不区分master slave） 集群实现： ex. server port is 6379; another tcp port 16379(adding 10000) used as cluster bus(故障检查，配置更新 etc) hash slot， 0-16383，集群负责维护node和slot和value的映射， Consistency guarantees：无法保证strong guarantee, Basically there is a trade-off to take between performance and consistency. 集群配置：1.依赖软件： ruby-2.3.1.tar.gz rubygem-2.6.6.zip redis-3.3.1.gem redis-3.2.3 高可用： 集群包含多个组， 每个组由一个master和至少1个slave组成； 同组内的结点的数据一致 集群不可用: 任意master不可用&amp;&amp;无slave， 此时集群的slot[0-16383]不完全覆盖参数：cluster-require-full-coverage 默认配置为yes，部分宕机 其他节点不对外服务 使用 集群是一个整体，数据按key的slot计算方法 分配到不同node的不同slot， 当client访问任意一个集群node，get某个key时，如果此key不在此node，则返回 该key所属的redirect信息：MOVED xxx ip:port client如果随机访问集群的某个node，很大概率需要通过redirect信息 再次访问正确的node这样处理的效率很低，正确的处理方式是client记录集群的node slot分配信息，每次访问时按照key计算的slot 访问具体的某个node。参考：spring-data-redis中的实现机制ClusterTopology类中 getKeyServingMasterNode 可以获取key对应的nodeClusterSlotHashUtil类中 calculateSlot实现了redis的slot分配策略 jedis模块实现了java连接redis的具体实现，redis的接口命令在此模块中都有实现 piple大量数据加载，通过数据文件形式执行cat data.txt | redis-cli –pipe partitionredis partition 和数据库分库类似， 将数据分布在多个实体上 需要解决的问题是： 键值分配规则 一般的规则包括： 按范围分段 键值 取模（字符键值可以使用crc32等方法转换为数字） 实现方式： 客户端分part 代理负责： TwemProxy 轮询 Disadvantage： 多key操作不适用 多key事物不适用 持久化工作复杂 扩容策略 用于数据存储or cache If Redis is used as a cache scaling up and down using consistent hashing is easy. If Redis is used as a store, a fixed keys-to-nodes map is used, so the number of nodes must be fixed and cannot vary. Otherwise, a system is needed that is able to rebalance keys between nodes when nodes are added or removed, and currently only Redis Cluster is able to do this - Redis Cluster is generally available and production-ready as of April 1st, 2015. 实践策略： config123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990991992993994995996997998999100010011002100310041005100610071008100910101011101210131014101510161017101810191020102110221023102410251026102710281029103010311032103310341035103610371038103910401041104210431044104510461047104810491050105110521053105410551056105710581059106010611062106310641065106610671068106910701071107210731074107510761077107810791080108110821083108410851086108710881089109010911092109310941095109610971098109911001101110211031104110511061107##redis配置详解# Redis configuration file example.## Note that in order to read the configuration file, Redis must be# started with the file path as first argument:## ./redis-server /path/to/redis.conf# Note on units: when memory size is needed, it is possible to specify# it in the usual form of 1k 5GB 4M and so forth:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.################################## INCLUDES ##################################################################### 包含 #################################### Include one or more other config files here. This is useful if you# have a standard template that goes to all Redis servers but also need# to customize a few per-server settings. Include files can include# other files, so use this wisely.## Notice option \"include\" won't be rewritten by command \"CONFIG REWRITE\"# from admin or Redis Sentinel. Since Redis always uses the last processed# line as value of a configuration directive, you'd better put includes# at the beginning of this file to avoid overwriting config change at runtime.## If instead you are interested in using includes to override configuration# options, it is better to use include as the last line.## 假如说你有一个可用于所有的 redis server 的标准配置模板，# 但针对某些 server 又需要一些个性化的设置，# 你可以使用 include 来包含一些其他的配置文件，这对你来说是非常有用的。## 但是要注意哦，include 是不能被 config rewrite 命令改写的# 由于 redis 总是以最后的加工线作为一个配置指令值，所以你最好是把 include 放在这个文件的最前面，# 以避免在运行时覆盖配置的改变，相反，你就把它放在后面# include /path/to/local.conf# include /path/to/other.conf################################ GENERAL ##################################################################### 常用 ###################################### By default Redis does not run as a daemon. Use 'yes' if you need it.# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.# 默认情况下 redis 不是作为守护进程运行的，如果你想让它在后台运行，你就把它改成 yes。# 当redis作为守护进程运行的时候，它会写一个 pid 到 /var/run/redis.pid 文件里面。daemonize yes# When running daemonized, Redis writes a pid file in /var/run/redis.pid by# default. You can specify a custom pid file location here.# 当 Redis 以守护进程的方式运行的时候，Redis 默认会把 pid 文件放在/var/run/redis.pid# 可配置到其他地址，当运行多个 redis 服务时，需要指定不同的 pid 文件和端口# 指定存储Redis进程号的文件路径pidfile /var/run/redis.pid# Accept connections on the specified port, default is 6379.# If port 0 is specified Redis will not listen on a TCP socket.# 端口，默认端口是6379，生产环境中建议更改端口号，安全性更高# 如果你设为 0 ，redis 将不在 socket 上监听任何客户端连接。port 9966# TCP listen() backlog.## In high requests-per-second environments you need an high backlog in order# to avoid slow clients connections issues. Note that the Linux kernel# will silently truncate it to the value of /proc/sys/net/core/somaxconn so# make sure to raise both the value of somaxconn and tcp_max_syn_backlog# in order to get the desired effect.# TCP 监听的最大容纳数量# 此参数确定了TCP连接中已完成队列(完成三次握手之后)的长度，# 当系统并发量大并且客户端速度缓慢的时候，你需要把这个值调高以避免客户端连接缓慢的问题。# Linux 内核会一声不响的把这个值缩小成 /proc/sys/net/core/somaxconn 对应的值，默认是511，而Linux的默认参数值是128。# 所以可以将这二个参数一起参考设定，你以便达到你的预期。# tcp-backlog 511# By default Redis listens for connections from all the network interfaces# available on the server. It is possible to listen to just one or multiple# interfaces using the \"bind\" configuration directive, followed by one or# more IP addresses.## Examples:## bind 192.168.1.100 10.0.0.1# 有时候为了安全起见，redis一般都是监听127.0.0.1 但是有时候又有同网段能连接的需求，当然可以绑定0.0.0.0 用iptables来控制访问权限，或者设置redis访问密码来保证数据安全# 不设置将处理所有请求,建议生产环境中设置，有个误区：bind是用来限制外网IP访问的，其实不是，限制外网ip访问可以通过iptables；如：-A INPUT -s 10.10.1.0/24 -p tcp -m state --state NEW -m tcp --dport 9966 -j ACCEPT ；# 实际上，bind ip 绑定的是redis所在服务器网卡的ip，当然127.0.0.1也是可以的#如果绑定一个外网ip，就会报错：Creating Server TCP listening socket xxx.xxx.xxx.xxx:9966: bind: Cannot assign requested address# bind 127.0.0.1bind 127.0.0.1 10.10.1.3# 假设绑定是以上ip，使用 netstat -anp|grep 9966 会发现，这两个ip被bind，其中10.10.1.3是服务器网卡的ip# tcp 0 0 10.10.1.3:9966 0.0.0.0:* LISTEN 11188/redis-server # tcp 0 0 127.0.0.1:9966 0.0.0.0:* LISTEN 11188/redis-server # Specify the path for the Unix socket that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /tmp/redis.sock# unixsocketperm 700# Close the connection after a client is idle for N seconds (0 to disable)# 客户端和Redis服务端的连接超时时间，默认是0，表示永不超时。timeout 0# TCP keepalive.## If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence# of communication. This is useful for two reasons:## 1) Detect dead peers.# 2) Take the connection alive from the point of view of network# equipment in the middle.## On Linux, the specified value (in seconds) is the period used to send ACKs.# Note that to close the connection the double of the time is needed.# On other kernels the period depends on the kernel configuration.## A reasonable value for this option is 60 seconds.# tcp 心跳包。## 如果设置为非零，则在与客户端缺乏通讯的时候使用 SO_KEEPALIVE 发送 tcp acks 给客户端。# 这个之所有有用，主要由两个原因：## 1) 防止死的 peers# 2) Take the connection alive from the point of view of network# equipment in the middle.## 推荐一个合理的值就是60秒tcp-keepalive 0# Specify the server verbosity level.# This can be one of:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)# 日志记录等级，4个可选值debug,verbose,notice,warning# 可以是下面的这些值：# debug (适用于开发或测试阶段)# verbose (many rarely useful info, but not a mess like the debug level)# notice (适用于生产环境)# warning (仅仅一些重要的消息被记录)loglevel notice# Specify the log file name. Also the empty string can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/null#配置 log 文件地址,默认打印在命令行终端的窗口上，也可设为/dev/null屏蔽日志、logfile \"/data/logs/redis/redis.log\"# To enable logging to the system logger, just set 'syslog-enabled' to yes,# and optionally update the other syslog parameters to suit your needs.# 要想把日志记录到系统日志，就把它改成 yes，# 也可以可选择性的更新其他的syslog 参数以达到你的要求# syslog-enabled no# Specify the syslog identity.# 设置 syslog 的 identity。# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and 'databases'-1# 可用的数据库数，默认值为16，默认数据库为0，数据库范围在0-（database-1）之间databases 16################################ SNAPSHOTTING ################################################################ 快照 ################################## Save the DB on disk:## save &lt;seconds&gt; &lt;changes&gt;## Will save the DB if both the given number of seconds and the given# number of write operations against the DB occurred.## In the example below the behaviour will be to save:# after 900 sec (15 min) if at least 1 key changed# after 300 sec (5 min) if at least 10 keys changed# after 60 sec if at least 10000 keys changed## Note: you can disable saving completely by commenting out all \"save\" lines.## It is also possible to remove all the previously configured save# points by adding a save directive with a single empty string argument# like in the following example:## save \"\"# 在 900 秒内最少有 1 个 key 被改动，或者 300 秒内最少有 10 个 key 被改动，又或者 60 秒内最少有 1000 个 key 被改动，以上三个条件随便满足一个，就触发一次保存操作。# if(在60秒之内有10000个keys发生变化时)&#123;# 进行镜像备份# &#125;else if(在300秒之内有10个keys发生了变化)&#123;# 进行镜像备份# &#125;else if(在900秒之内有1个keys发生了变化)&#123;# 进行镜像备份# &#125;save 900 1save 300 10save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some#:/ disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.# 默认情况下，如果 redis 最后一次的后台保存失败，redis 将停止接受写操作，# 这样以一种强硬的方式让用户知道数据不能正确的持久化到磁盘，# 否则就会没人注意到灾难的发生。## 如果后台保存进程重新启动工作了，redis 也将自动的允许写操作。## 然而你要是安装了靠谱的监控，你可能不希望 redis 这样做，那你就改成 no 好stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases?# For default that's set to 'yes' as it's almost always a win.# If you want to save some CPU in the saving child set it to 'no' but# the dataset will likely be bigger if you have compressible values or keys.# 在进行备份时,是否进行压缩# 是否在 dump .rdb 数据库的时候使用 LZF 压缩字符串# 默认都设为 yes# 如果你希望保存子进程节省点 cpu ，你就设置它为 no ，# 不过这个数据集可能就会比较大rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check. # 读取和写入的时候是否支持CRC64校验，默认是开启的rdbchecksum yes# The filename where to dump the DB# 备份文件的文件名dbfilename dump.rdb# The working directory.## The DB will be written inside this directory, with the filename specified# above using the 'dbfilename' configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.# 数据库备份的文件放置的路径# 路径跟文件名分开配置是因为 Redis 备份时，先会将当前数据库的状态写入到一个临时文件# 等备份完成时，再把该临时文件替换为上面所指定的文件# 而临时文件和上面所配置的备份文件都会放在这个指定的路径当中# 默认值为 ./dir /data/data/redis/################################# REPLICATION ################################################################## 主从复制 ################################## Master-Slave replication. Use slaveof to make a Redis instance a copy of# another Redis server. A few things to understand ASAP about Redis replication.## 1) Redis replication is asynchronous, but you can configure a master to# stop accepting writes if it appears to be not connected with at least# a given number of slaves.# 2) Redis slaves are able to perform a partial resynchronization with the# master if the replication link is lost for a relatively small amount of# time. You may want to configure the replication backlog size (see the next# sections of this file) with a sensible value depending on your needs.# 3) Replication is automatic and does not need user intervention. After a# network partition slaves automatically try to reconnect to masters# and resynchronize with them.## 设置该数据库为其他数据库的从数据库# slaveof &lt;masterip&gt; &lt;masterport&gt; 当本机为从服务时，设置主服务的IP及端口# slaveof &lt;masterip&gt; &lt;masterport&gt;# If the master is password protected (using the \"requirepass\" configuration# directive below) it is possible to tell the slave to authenticate before# starting the replication synchronization process, otherwise the master will# refuse the slave request.## 指定与主数据库连接时需要的密码验证# masterauth &lt;master-password&gt; 当本机为从服务时，设置访问master服务器的密码# masterauth &lt;master-password&gt;# When a slave loses its connection with the master, or when the replication# is still in progress, the slave can act in two different ways:## 1) if slave-serve-stale-data is set to 'yes' (the default) the slave will# still reply to client requests, possibly with out of date data, or the# data set may just be empty if this is the first synchronization.## 2) if slave-serve-stale-data is set to 'no' the slave will reply with# an error \"SYNC with master in progress\" to all the kind of commands# but to INFO and SLAVEOF.## 当slave服务器和master服务器失去连接后，或者当数据正在复制传输的时候，如果此参数值设置“yes”，slave服务器可以继续接受客户端的请求，否则，会返回给请求的客户端如下信息“SYNC with master in progress”,除了INFO，SLAVEOF这两个命令slave-serve-stale-data yes# You can configure a slave instance to accept writes or not. Writing against# a slave instance may be useful to store some ephemeral data (because data# written on a slave will be easily deleted after resync with the master) but# may also cause problems if clients are writing to it because of a# misconfiguration.## Since Redis 2.6 by default slaves are read-only.## Note: read only slaves are not designed to be exposed to untrusted clients# on the internet. It's just a protection layer against misuse of the instance.# Still a read only slave exports by default all the administrative commands# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve# security of read only slaves using 'rename-command' to shadow all the# administrative / dangerous commands.# 是否允许slave服务器节点只提供读服务slave-read-only yes# Replication SYNC strategy: disk or socket.## -------------------------------------------------------# WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY# -------------------------------------------------------## New slaves and reconnecting slaves that are not able to continue the replication# process just receiving differences, need to do what is called a \"full# synchronization\". An RDB file is transmitted from the master to the slaves.# The transmission can happen in two different ways:## 1) Disk-backed: The Redis master creates a new process that writes the RDB# file on disk. Later the file is transferred by the parent# process to the slaves incrementally.# 2) Diskless: The Redis master creates a new process that directly writes the# RDB file to slave sockets, without touching the disk at all.## With disk-backed replication, while the RDB file is generated, more slaves# can be queued and served with the RDB file as soon as the current child producing# the RDB file finishes its work. With diskless replication instead once# the transfer starts, new slaves arriving will be queued and a new transfer# will start when the current one terminates.## When diskless replication is used, the master waits a configurable amount of# time (in seconds) before starting the transfer in the hope that multiple slaves# will arrive and the transfer can be parallelized.## With slow disks and fast (large bandwidth) networks, diskless replication# works better.repl-diskless-sync no# When diskless replication is enabled, it is possible to configure the delay# the server waits in order to spawn the child that transfers the RDB via socket# to the slaves.## This is important since once the transfer starts, it is not possible to serve# new slaves arriving, that will be queued for the next RDB transfer, so the server# waits a delay in order to let more slaves arrive.## The delay is specified in seconds, and by default is 5 seconds. To disable# it entirely just set it to 0 seconds and the transfer will start ASAP.repl-diskless-sync-delay 5# Slaves send PINGs to server in a predefined interval. It's possible to change# this interval with the repl_ping_slave_period option. The default value is 10# seconds.## Slaves 在一个预定义的时间间隔内发送 ping 命令到 server 。# 你可以改变这个时间间隔。默认为 10 秒。# repl-ping-slave-period 10# The following option sets the replication timeout for:## 1) Bulk transfer I/O during SYNC, from the point of view of slave.# 2) Master timeout from the point of view of slaves (data, pings).# 3) Slave timeout from the point of view of masters (REPLCONF ACK pings).## It is important to make sure that this value is greater than the value# specified for repl-ping-slave-period otherwise a timeout will be detected# every time there is low traffic between the master and the slave.## 设置主从复制过期时间# 这个值一定要比 repl-ping-slave-period 大# repl-timeout 60# Disable TCP_NODELAY on the slave socket after SYNC?## If you select \"yes\" Redis will use a smaller number of TCP packets and# less bandwidth to send data to slaves. But this can add a delay for# the data to appear on the slave side, up to 40 milliseconds with# Linux kernels using a default configuration.## If you select \"no\" the delay for data to appear on the slave side will# be reduced but more bandwidth will be used for replication.## By default we optimize for low latency, but in very high traffic conditions# or when the master and slaves are many hops away, turning this to \"yes\" may# be a good idea.# 指定向slave同步数据时，是否禁用socket的NO_DELAY选 项。若配置为“yes”，则禁用NO_DELAY，则TCP协议栈会合并小包统一发送，这样可以减少主从节点间的包数量并节省带宽，但会增加数据同步到 slave的时间。若配置为“no”，表明启用NO_DELAY，则TCP协议栈不会延迟小包的发送时机，这样数据同步的延时会减少，但需要更大的带宽。 通常情况下，应该配置为no以降低同步延时，但在主从节点间网络负载已经很高的情况下，可以配置为yes。repl-disable-tcp-nodelay no# Set the replication backlog size. The backlog is a buffer that accumulates# slave data when slaves are disconnected for some time, so that when a slave# wants to reconnect again, often a full resync is not needed, but a partial# resync is enough, just passing the portion of data the slave missed while# disconnected.## The bigger the replication backlog, the longer the time the slave can be# disconnected and later be able to perform a partial resynchronization.## The backlog is only allocated once there is at least a slave connected.## 设置主从复制容量大小。这个 backlog 是一个用来在 slaves 被断开连接时# 存放 slave 数据的 buffer，所以当一个 slave 想要重新连接，通常不希望全部重新同步，# 只是部分同步就够了，仅仅传递 slave 在断开连接时丢失的这部分数据。## The biggest the replication backlog, the longer the time the slave can be# disconnected and later be able to perform a partial resynchronization.# 这个值越大，salve 可以断开连接的时间就越长。# repl-backlog-size 1mb# After a master has no longer connected slaves for some time, the backlog# will be freed. The following option configures the amount of seconds that# need to elapse, starting from the time the last slave disconnected, for# the backlog buffer to be freed.## A value of 0 means to never release the backlog.## 在某些时候，master 不再连接 slaves，backlog 将被释放。# 如果设置为 0 ，意味着绝不释放 backlog 。# repl-backlog-ttl 3600# The slave priority is an integer number published by Redis in the INFO output.# It is used by Redis Sentinel in order to select a slave to promote into a# master if the master is no longer working correctly.## A slave with a low priority number is considered better for promotion, so# for instance if there are three slaves with priority 10, 100, 25 Sentinel will# pick the one with priority 10, that is the lowest.## However a special priority of 0 marks the slave as not able to perform the# role of master, so a slave with priority of 0 will never be selected by# Redis Sentinel for promotion.## By default the priority is 100.# 指定slave的优先级。在不只1个slave存在的部署环境下，当master宕机时，Redis# Sentinel会将priority值最小的slave提升为master。# 这个值越小，就越会被优先选中，需要注意的是，# 若该配置项为0，则对应的slave永远不会自动提升为master。slave-priority 100# It is possible for a master to stop accepting writes if there are less than# N slaves connected, having a lag less or equal than M seconds.## The N slaves need to be in \"online\" state.## The lag in seconds, that must be &lt;= the specified value, is calculated from# the last ping received from the slave, that is usually sent every second.## This option does not GUARANTEE that N replicas will accept the write, but# will limit the window of exposure for lost writes in case not enough slaves# are available, to the specified number of seconds## For example to require at least 3 slaves with a lag &lt;= 10 seconds use:## min-slaves-to-write 3# min-slaves-max-lag 10## Setting one or the other to 0 disables the feature.## By default min-slaves-to-write is set to 0 (feature disabled) and# min-slaves-max-lag is set to 10.################################## SECURITY ##################################################################### 安全 #################################### Require clients to issue AUTH &lt;PASSWORD&gt; before processing any other# commands. This might be useful in environments in which you do not trust# others with access to the host running redis-server.## This should stay commented out for backward compatibility and because most# people do not need auth (e.g. they run their own servers).## Warning: since Redis is pretty fast an outside user can try up to# 150k passwords per second against a good box. This means that you should# use a very strong password otherwise it will be very easy to break.## 设置连接redis的密码# redis速度相当快，一个外部用户在一秒钟进行150K次密码尝试，需指定强大的密码来防止暴力破解requirepass set_enough_strong_passwd# Command renaming.## It is possible to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# hard to guess so that it will still be available for internal-use tools# but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possible to completely kill a command by renaming it into# an empty string:## rename-command CONFIG \"\"## Please note that changing the name of commands that are logged into the# AOF file or transmitted to slaves may cause problems.# 重命名一些高危命令，用来禁止高危命令rename-command FLUSHALL ZYzv6FOBdwflW2nXrename-command CONFIG aI7zwm1GDzMMrEirename-command EVAL S9UHPKEpSvUJMMrename-command FLUSHDB D60FPVDJuip7gy6l################################### LIMITS ####################################################################### 限制 ##################################### Set the max number of connected clients at the same time. By default# this limit is set to 10000 clients, however if the Redis server is not# able to configure the process file limit to allow for the specified limit# the max number of allowed clients is set to the current file limit# minus 32 (as Redis reserves a few file descriptors for internal uses).## Once the limit is reached Redis will close all the new connections sending# an error 'max number of clients reached'.## 限制同时连接的客户数量,默认是10000# 当连接数超过这个值时，redis 将不再接收其他连接请求，客户端尝试连接时将收到 error 信息# maxclients 10000# Don't use more memory than the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys# according to the eviction policy selected (see maxmemory-policy).## If Redis can't remove keys according to the policy, or if the policy is# set to 'noeviction', Redis will start to reply with errors to commands# that would use more memory, like SET, LPUSH, and so on, and will continue# to reply to read-only commands like GET.## This option is usually useful when using Redis as an LRU cache, or to set# a hard memory limit for an instance (using the 'noeviction' policy).## WARNING: If you have slaves attached to an instance with maxmemory on,# the size of the output buffers needed to feed the slaves are subtracted# from the used memory count, so that network problems / resyncs will# not trigger a loop where keys are evicted, and in turn the output# buffer of slaves is full with DELs of keys evicted triggering the deletion# of more keys, and so forth until the database is completely emptied.## In short... if you have slaves attached it is suggested that you set a lower# limit for maxmemory so that there is some free RAM on the system for slave# output buffers (but this is not needed if the policy is 'noeviction').## 设置redis能够使用的最大内存。# 达到最大内存设置后，Redis会先尝试清除已到期或即将到期的Key（设置过expire信息的key）# 在删除时,按照过期时间进行删除，最早将要被过期的key将最先被删除# 如果已到期或即将到期的key删光，仍进行set操作，那么将返回错误# 此时redis将不再接收写请求,只接收get请求。# maxmemory的设置比较适合于把redis当作于类似memcached 的缓存来使用# maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached. You can select among five behaviors:## volatile-lru -&gt; remove the key with an expire set using an LRU algorithm# allkeys-lru -&gt; remove any key according to the LRU algorithm# volatile-random -&gt; remove a random key with an expire set# allkeys-random -&gt; remove a random key, any key# volatile-ttl -&gt; remove the key with the nearest expire time (minor TTL)# noeviction -&gt; don't expire at all, just return an error on write operations## Note: with any of the above policies, Redis will return an error on write# operations, when there are no suitable keys for eviction.## At the date of writing these commands are: set setnx setex append# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby# getset mset msetnx exec sort## The default is:## maxmemory-policy noeviction# LRU and minimal TTL algorithms are not precise algorithms but approximated# algorithms (in order to save memory), so you can tune it for speed or# accuracy. For default Redis will check five keys and pick the one that was# used less recently, you can change the sample size using the following# configuration directive.## The default of 5 produces good enough results. 10 Approximates very closely# true LRU but costs a bit more CPU. 3 is very fast but not very accurate.## maxmemory-samples 5############################## APPEND ONLY MODE ################################ By default Redis asynchronously dumps the dataset on disk. This mode is# good enough in many applications, but an issue with the Redis process or# a power outage may result into a few minutes of writes lost (depending on# the configured save points).## The Append Only File is an alternative persistence mode that provides# much better durability. For instance using the default data fsync policy# (see later in the config file) Redis can lose just one second of writes in a# dramatic event like a server power outage, or a single write if something# wrong with the Redis process itself happens, but the operating system is# still running correctly.## AOF and RDB persistence can be enabled at the same time without problems.# If the AOF is enabled on startup Redis will load the AOF, that is the file# with the better durability guarantees.## Please check http://redis.io/topics/persistence for more information.# redis 默认每次更新操作后会在后台异步的把数据库镜像备份到磁盘，但该备份非常耗时，且备份不宜太频繁# redis 同步数据文件是按上面save条件来同步的# 如果发生诸如拉闸限电、拔插头等状况,那么将造成比较大范围的数据丢失# 所以redis提供了另外一种更加高效的数据库备份及灾难恢复方式# 开启append only 模式后,redis 将每一次写操作请求都追加到appendonly.aof 文件中# redis重新启动时,会从该文件恢复出之前的状态。# 但可能会造成 appendonly.aof 文件过大，所以redis支持BGREWRITEAOF 指令，对appendonly.aof重新整理,默认是不开启的。appendonly no# The name of the append only file (default: \"appendonly.aof\")# 默认为appendonly.aof。appendfilename \"appendonly.aof\"# The fsync() call tells the Operating System to actually write data on disk# instead of waiting for more data in the output buffer. Some OS will really flush# data on disk, some other OS will just try to do it ASAP.## Redis supports three different modes:## no: don't fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.## The default is \"everysec\", as that's usually the right compromise between# speed and data safety. It's up to you to understand if you can relax this to# \"no\" that will let the operating system flush the output buffer when# it wants, for better performances (but if you can live with the idea of# some data loss consider the default persistence mode that's snapshotting),# or on the contrary, use \"always\" that's very slow but a bit safer than# everysec.## More details please check the following article:# http://antirez.com/post/redis-persistence-demystified.html## If unsure, use \"everysec\".# 设置对 appendonly.aof 文件进行同步的频率,有三种选择always、everysec、no，默认是everysec表示每秒同步一次。# always 表示每次有写操作都进行同步,everysec 表示对写操作进行累积,每秒同步一次。# no表示等操作系统进行数据缓存同步到磁盘，都进行同步,everysec 表示对写操作进行累积,每秒同步一次# appendfsync always# appendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background# saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations# Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block# our synchronous write(2) call.## In order to mitigate this problem it's possible to use the following option# that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.## This means that while another child is saving, the durability of Redis is# the same as \"appendfsync none\". In practical terms, this means that it is# possible to lose up to 30 seconds of log in the worst scenario (with the# default Linux settings).## If you have latency problems turn this to \"yes\". Otherwise leave it as# \"no\" that is the safest pick from the point of view of durability.# 指定是否在后台aof文件rewrite期间调用fsync，默认为no，表示要调用fsync（无论后台是否有子进程在刷盘）。Redis在后台写RDB文件或重写afo文件期间会存在大量磁盘IO，此时，在某些linux系统中，调用fsync可能会阻塞。no-appendfsync-on-rewrite yes# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size grows by the specified percentage.## This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (if no rewrite has happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a percentage of zero in order to disable the automatic AOF# rewrite feature.# 指定Redis重写aof文件的条件，默认为100，表示与上次rewrite的aof文件大小相比，当前aof文件增长量超过上次afo文件大小的100%时，就会触发background rewrite。若配置为0，则会禁用自动rewriteauto-aof-rewrite-percentage 100# 指定触发rewrite的aof文件大小。若aof文件小于该值，即使当前文件的增量比例达到auto-aof-rewrite-percentage的配置值，也不会触发自动rewrite。即这两个配置项同时满足时，才会触发rewrite。auto-aof-rewrite-min-size 64mb# An AOF file may be found to be truncated at the end during the Redis# startup process, when the AOF data gets loaded back into memory.# This may happen when the system where Redis is running# crashes, especially when an ext4 filesystem is mounted without the# data=ordered option (however this can't happen when Redis itself# crashes or aborts but the operating system still works correctly).## Redis can either exit with an error when this happens, or load as much# data as possible (the default now) and start if the AOF file is found# to be truncated at the end. The following option controls this behavior.## If aof-load-truncated is set to yes, a truncated AOF file is loaded and# the Redis server starts emitting a log to inform the user of the event.# Otherwise if the option is set to no, the server aborts with an error# and refuses to start. When the option is set to no, the user requires# to fix the AOF file using the \"redis-check-aof\" utility before to restart# the server.## Note that if the AOF file will be found to be corrupted in the middle# the server will still exit with an error. This option only applies when# Redis will try to read more data from the AOF file but not enough bytes# will be found.aof-load-truncated yes################################ LUA SCRIPTING ################################ Max execution time of a Lua script in milliseconds.## If the maximum execution time is reached Redis will log that a script is# still in execution after the maximum allowed time and will start to# reply to queries with an error.## When a long running script exceeds the maximum execution time only the# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be# used to stop a script that did not yet called write commands. The second# is the only way to shut down the server in the case a write command was# already issued by the script but the user doesn't want to wait for the natural# termination of the script.## Set it to 0 or a negative value for unlimited execution without warnings.# 一个Lua脚本最长的执行时间，单位为毫秒，如果为0或负数表示无限执行时间，默认为5000lua-time-limit 5000################################ REDIS CLUSTER ################################# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++# WARNING EXPERIMENTAL: Redis Cluster is considered to be stable code, however# in order to mark it as \"mature\" we need to wait for a non trivial percentage# of users to deploy it in production.# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++## Normal Redis instances can't be part of a Redis Cluster; only nodes that are# started as cluster nodes can. In order to start a Redis instance as a# cluster node enable the cluster support uncommenting the following:## cluster-enabled yes# Every cluster node has a cluster configuration file. This file is not# intended to be edited by hand. It is created and updated by Redis nodes.# Every Redis Cluster node requires a different cluster configuration file.# Make sure that instances running in the same system do not have# overlapping cluster configuration file names.## cluster-config-file nodes-6379.conf# Cluster node timeout is the amount of milliseconds a node must be unreachable# for it to be considered in failure state.# Most other internal time limits are multiple of the node timeout.## cluster-node-timeout 15000# A slave of a failing master will avoid to start a failover if its data# looks too old.## There is no simple way for a slave to actually have a exact measure of# its \"data age\", so the following two checks are performed:## 1) If there are multiple slaves able to failover, they exchange messages# in order to try to give an advantage to the slave with the best# replication offset (more data from the master processed).# Slaves will try to get their rank by offset, and apply to the start# of the failover a delay proportional to their rank.## 2) Every single slave computes the time of the last interaction with# its master. This can be the last ping or command received (if the master# is still in the \"connected\" state), or the time that elapsed since the# disconnection with the master (if the replication link is currently down).# If the last interaction is too old, the slave will not try to failover# at all.## The point \"2\" can be tuned by user. Specifically a slave will not perform# the failover if, since the last interaction with the master, the time# elapsed is greater than:## (node-timeout * slave-validity-factor) + repl-ping-slave-period## So for example if node-timeout is 30 seconds, and the slave-validity-factor# is 10, and assuming a default repl-ping-slave-period of 10 seconds, the# slave will not try to failover if it was not able to talk with the master# for longer than 310 seconds.## A large slave-validity-factor may allow slaves with too old data to failover# a master, while a too small value may prevent the cluster from being able to# elect a slave at all.## For maximum availability, it is possible to set the slave-validity-factor# to a value of 0, which means, that slaves will always try to failover the# master regardless of the last time they interacted with the master.# (However they'll always try to apply a delay proportional to their# offset rank).## Zero is the only value able to guarantee that when all the partitions heal# the cluster will always be able to continue.## cluster-slave-validity-factor 10# Cluster slaves are able to migrate to orphaned masters, that are masters# that are left without working slaves. This improves the cluster ability# to resist to failures as otherwise an orphaned master can't be failed over# in case of failure if it has no working slaves.## Slaves migrate to orphaned masters only if there are still at least a# given number of other working slaves for their old master. This number# is the \"migration barrier\". A migration barrier of 1 means that a slave# will migrate only if there is at least 1 other working slave for its master# and so forth. It usually reflects the number of slaves you want for every# master in your cluster.## Default is 1 (slaves migrate only if their masters remain with at least# one slave). To disable migration just set it to a very large value.# A value of 0 can be set but is useful only for debugging and dangerous# in production.## cluster-migration-barrier 1# By default Redis Cluster nodes stop accepting queries if they detect there# is at least an hash slot uncovered (no available node is serving it).# This way if the cluster is partially down (for example a range of hash slots# are no longer covered) all the cluster becomes, eventually, unavailable.# It automatically returns available as soon as all the slots are covered again.## However sometimes you want the subset of the cluster which is working,# to continue to accept queries for the part of the key space that is still# covered. In order to do so, just set the cluster-require-full-coverage# option to no.## cluster-require-full-coverage yes# In order to setup your cluster make sure to read the documentation# available at http://redis.io web site.################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).## You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.slowlog-max-len 128################################ LATENCY MONITOR ############################### The Redis latency monitoring subsystem samples different operations# at runtime in order to collect data related to possible sources of# latency of a Redis instance.## Via the LATENCY command this information is available to the user that can# print graphs and obtain reports.## The system only logs operations that were performed in a time equal or# greater than the amount of milliseconds specified via the# latency-monitor-threshold configuration directive. When its value is set# to zero, the latency monitor is turned off.## By default latency monitoring is disabled since it is mostly not needed# if you don't have latency issues, and collecting data has a performance# impact, that while very small, can be measured under big load. Latency# monitoring can easily be enabled at runtime using the command# \"CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;\" if needed.latency-monitor-threshold 0############################# EVENT NOTIFICATION ############################### Redis can notify Pub/Sub clients about events happening in the key space.# This feature is documented at http://redis.io/topics/notifications## For instance if keyspace events notification is enabled, and a client# performs a DEL operation on key \"foo\" stored in the Database 0, two# messages will be published via Pub/Sub:## PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo## It is possible to select the events that Redis will notify among a set# of classes. Every class is identified by a single character:## K Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.# E Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.# g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...# $ String commands# l List commands# s Set commands# h Hash commands# z Sorted set commands# x Expired events (events generated every time a key expires)# e Evicted events (events generated when a key is evicted for maxmemory)# A Alias for g$lshzxe, so that the \"AKE\" string means all the events.## The \"notify-keyspace-events\" takes as argument a string that is composed# of zero or multiple characters. The empty string means that notifications# are disabled.## Example: to enable list and generic events, from the point of view of the# event name, use:## notify-keyspace-events Elg## Example 2: to get the stream of the expired keys subscribing to channel# name __keyevent@0__:expired use:## notify-keyspace-events Ex## By default all notifications are disabled because most users don't need# this feature and the feature has some overhead. Note that if you don't# specify at least one of K or E, no events will be delivered.notify-keyspace-events \"\"############################### ADVANCED CONFIG ################################ Hashes are encoded using a memory efficient data structure when they have a# small number of entries, and the biggest entry does not exceed a given# threshold. These thresholds can be configured using the following directives.# 当hash中包含超过指定元素个数并且最大的元素没有超过临界时，# hash将以一种特殊的编码方式（大大减少内存使用）来存储，这里可以设置这两个临界值hash-max-ziplist-entries 512hash-max-ziplist-value 64# Similarly to hashes, small lists are also encoded in a special way in order# to save a lot of space. The special representation is only used when# you are under the following limits:# list数据类型多少节点以下会采用去指针的紧凑存储格式。# list数据类型节点值大小小于多少字节会采用紧凑存储格式。list-max-ziplist-entries 512list-max-ziplist-value 64# Sets have a special encoding in just one case: when a set is composed# of just strings that happen to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the# set in order to use this special memory saving encoding.# set数据类型内部数据如果全部是数值型，且包含多少节点以下会采用紧凑格式存储。set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in# order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:# zsort数据类型多少节点以下会采用去指针的紧凑存储格式。# zsort数据类型节点值大小小于多少字节会采用紧凑存储格式。zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog sparse representation bytes limit. The limit includes the# 16 bytes header. When an HyperLogLog using the sparse representation crosses# this limit, it is converted into the dense representation.## A value greater than 16000 is totally useless, since at that point the# dense representation is more memory efficient.## The suggested value is ~ 3000 in order to have the benefits of# the space efficient encoding without slowing down too much PFADD,# which is O(N) with the sparse encoding. The value can be raised to# ~ 10000 when CPU is not a concern, but space is, and the data set is# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.hll-sparse-max-bytes 3000# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation Redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into a hash table# that is rehashing, the more rehashing \"steps\" are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.## The default is to use this millisecond 10 times every second in order to# actively rehash the main dictionaries, freeing memory when possible.## If unsure:# use \"activerehashing no\" if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply from time to time# to queries with 2 milliseconds delay.## use \"activerehashing yes\" if you don't have such hard requirements but# want to free memory asap when possible.# Redis将在每100毫秒时使用1毫秒的CPU时间来对redis的hash表进行重新hash，可以降低内存的使用# 当你的使用场景中，有非常严格的实时性需要，不能够接受Redis时不时的对请求有2毫秒的延迟的话，把这项配置为no。# 如果没有这么严格的实时性要求，可以设置为yes，以便能够尽可能快的释放内存activerehashing yes# The client output buffer limits can be used to force disconnection of clients# that are not reading data from the server fast enough for some reason (a# common reason is that a Pub/Sub client can't consume messages as fast as the# publisher can produce them).## The limit can be set differently for the three different classes of clients:## normal -&gt; normal clients including MONITOR clients# slave -&gt; slave clients# pubsub -&gt; clients subscribed to at least one pubsub channel or pattern## The syntax of every client-output-buffer-limit directive is the following:## client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;## A client is immediately disconnected once the hard limit is reached, or if# the soft limit is reached and remains reached for the specified number of# seconds (continuously).# So for instance if the hard limit is 32 megabytes and the soft limit is# 16 megabytes / 10 seconds, the client will get disconnected immediately# if the size of the output buffers reach 32 megabytes, but will also get# disconnected if the client reaches 16 megabytes and continuously overcomes# the limit for 10 seconds.## By default normal clients are not limited because they don't receive data# without asking (in a push way), but just after a request, so only# asynchronous clients may create a scenario where data is requested faster# than it can read.## Instead there is a default limit for pubsub and slave clients, since# subscribers and slaves receive data in a push fashion.## Both the hard or the soft limit can be disabled by setting them to zero.client-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# Redis calls an internal function to perform many background tasks, like# closing connections of clients in timeout, purging expired keys that are# never requested, and so forth.## Not all tasks are performed with the same frequency, but Redis checks for# tasks to perform according to the specified \"hz\" value.## By default \"hz\" is set to 10. Raising the value will use more CPU when# Redis is idle, but at the same time will make Redis more responsive when# there are many keys expiring at the same time, and timeouts may be# handled with more precision.## The range is between 1 and 500, however a value over 100 is usually not# a good idea. Most users should use the default of 10 and raise this up to# 100 only in environments where very low latency is required.hz 10# When a child rewrites the AOF file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.# aof rewrite过程中,是否采取增量文件同步策略,默认为“yes”。 rewrite过程中,每32M数据进行一次文件同步,这样可以减少aof大文件写入对磁盘的操作次数aof-rewrite-incremental-fsync yes# redis数据存储redis的存储分为内存存储、磁盘存储和log文件三部分，配置文件中有三个参数对其进行配置。save seconds updates，save配置，指出在多长时间内，有多少次更新操作，就将数据同步到数据文件。可多个条件配合，默认配置了三个条件。appendonly yes/no ，appendonly配置，指出是否在每次更新操作后进行日志记录，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为redis本身同步数据文件是按上面的save条件来同步的，所以有的数据会在一段时间内只存在于内存中。appendfsync no/always/everysec ，appendfsync配置，no表示等操作系统进行数据缓存同步到磁盘，always表示每次更新操作后手动调用fsync()将数据写到磁盘，everysec表示每秒同步一次。","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://www.sillystone.info/tags/redis/"}]},{"title":"阿里RocketMQ开发者见面会","date":"2017-07-03T04:21:17.000Z","path":"2017/07/03/阿里RocketMQ开发者见面会/","text":"故事自从上次被阿里面试虐过一次后，最近研究了一下流行的分布式系统技术架构。 阿里的rocketMQ最近加入了apache基金会阵营，也一下变得火了。 35度的北京， 会场满满的都是人（估计这么一屋子IT合起来的估值也不小） 会议组织的一般，节奏，内容，互动都没有很好。案例分享也没有含金量。 Flink Spark standalone架构 总结apache 开源项目 开源协议介绍（单位里也介绍过，了解更多了） 社区，相关参与角色的介绍 社区贡献决定了在项目中的地位 产品总结 消息处理：kafka VS RocketMQ 流式 实时/批量处理：flink/storm/spark stream VS spark/hadoopflink 已试用 框架： beam 分布式日志收集，处理：Flume 主题内容1、分布式消息引擎最佳实践主要分享分布式消息引擎的功能特性(顺序消息、SQL Filtering、 Batch、 LogAppender), 使用场景（金融交易、大数据等），以及生产实践的常用问题 2、分布式消息引擎性能优化及大数据生态主要分享百万吞吐与低延迟的奥秘，RocketMQ与Kafka的对比，阿里云上Kafka的优化与实施 周新宇：Apache RocketMQ PMC/Committer，2015年阿里巴巴性能挑战大赛冠军。目前主要负责消息引擎RocketMQ的性能优化，规范演进以及社区输出 3、基于分布式消息引擎的流计算本主题主要分享对流处理和消息系统的理解，以及在项目实践过程中流处理和消息系统集成的经验等。 王鑫：Apache Storm PMC/Committer，流处理技术专家，开源爱好者。5年高性能分布式系统研发经验。负责过分布式网络爬虫、内容聚合、用户行为分析、个性化推荐等大数据项目。目前主要兴趣在于分布式流处理引擎、分布式消息引擎等。对分布式系统的架构及调优有丰富经验。 storm作为流式计算的案例分享 4、RocketMQ-MySQL-Connector项目介绍主要分享RocketMQ-MySQL-Connector项目的应用场景，目前的进展情况以及未来的规划。 赵群：Apache RocketMQ Contributor，开源爱好者，多年高并发系统架构经验，目前从事通信平台建设工作，主要关注分布式系统、网络通信等领域。 可以用于 数据库复制/数据在线迁移的工具， 介绍了实践案例：是基于数据库日志的事物读取 并通过RocketMQ 发送给下游的应用 Apache Way：如何打造世界级软件产品主要分享对Apache Way的理解，建设Apache社区的经验，以及如何打造一款能在世界范围内被认可的软件产品","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://www.sillystone.info/tags/大数据/"},{"name":"分布式","slug":"分布式","permalink":"http://www.sillystone.info/tags/分布式/"},{"name":"消息服务","slug":"消息服务","permalink":"http://www.sillystone.info/tags/消息服务/"}]},{"title":"奇葩说","date":"2017-06-24T15:58:55.000Z","path":"2017/06/24/奇葩说/","text":"朋友介绍，看了《奇葩说》 第四季的最后一场 辩论本身题目我们终将变成我们讨厌的人； 成为讨厌的人是坏事吗？ 辩论要点 讨厌的定义 讨厌不是憎恨，讨厌的事不是大家所共识的堕落的事， 讨厌是个人在特定时段的感情 讨厌有轻重，我认为：讨厌的事就是做了超过自己底线的事，而底线，每个人不同，每个时间段也不同 我们 我们，意指所有的人，不是个人的个别特例；这点将所有的极端举例都驳倒 （老罗） 终将 代表着时间跨度 夹杂着感情，无奈，悲观感情 变 观念的变化 对于变化的接纳 命题:这种变化是否是坏事 变化是否有黑白之分 变化的定义决定了褒贬的成分。 （老罗把这种变化定义为成长，提出勇敢面对变化） 坏的界定我认为：坏是个人感情表达，好坏本身是黑白分明的，没有灰度的；对于自己是否可以容忍自己跨越底线这件事情本身，是不喜欢的，是坏的，这点大家应该是没有疑问的。 老罗回避了这点，而是给大家讲述了 对于你面对的这个坏事情 你要选择怎么处理，提出能够接纳变化，能够适应成长，对于不同的差异更能提高自己的容忍度，提高自己的适应力；但对于很重要的底线 我认为我们不应该随意修改，但重要不重要只有你知道 辩手观点总结黄执中： 强化了讨厌的程度，提供了强力的实例；跳出轮回，摆脱人性束缚的人才是伟大的人，不要自我和解马薇薇： 强调了个性受社会和现实的约束，好的社会尊重个人喜好，提供发展空间；社会是无情的命运是无情的，人成长就是坏事 蔡康永：讨厌是青年不成熟的想法，接纳变化是成熟的表现马东：事与愿违的事往往是正确的，而变成你讨厌的人这件事，也是事与愿违的事讨厌有很多场景，也有轻重之分，很难用这个词界定好坏；世事洞明 却不以事故待人老罗：成长是主观不断重建的过程，这个过程你需要不断地接纳变化，接受挑战 感思想的对抗比思想的陈述更值得思考奇袭之所以可行，在于观点之间没有强弱之分 摘万物皆裂痕，那又怎样，裂痕，那是光照进来的地方 成长， 就是你主观世界遇到 客观世界的那条沟。 掉进去了，叫挫折； 爬出来了，叫成长","categories":[{"name":"随笔","slug":"随笔","permalink":"http://www.sillystone.info/categories/随笔/"}],"tags":[{"name":"日记","slug":"日记","permalink":"http://www.sillystone.info/tags/日记/"}]},{"title":"需求分析培训","date":"2017-06-17T01:30:04.000Z","path":"2017/06/17/需求分析培训/","text":"价值需求宏观需求 功能需求流程-&gt;场景-&gt;功能 流程确定： 端到端业务需求分析： 起始端-接入系统开始（业务模型分析： 起始端-最终） 四类流程： 主业务流程（主业务） ：入住流程 变体业务流程（辅助业务）：换房，续房 支撑流程（服务，辅助）： 客房服务，查询，投诉 管理流程（控制）：押金不足 业务流程是分层的： 组织级流程： 展现部门间协助，部门维度（同级别组织） 部门级流程： 展现岗位间协作，岗位/系统 个人级流程 业务流程八个要素： 分工（规模，风险，专业） # 协作 活动 异常 规则 描述业务流程： 工具 跨职能流程图：商业建模标准，复用性强，用户接受，并行，异步支持差 活动图：UML标准，语义分析，强调行为流，强调活动内容 时序图：UML标准，强调行为流，强调协作，交互，技术常用 数据流图（上下文关系）：IDEF建模标准，强调数据流，未标识谁执行，计费类系统适合 现场出流程图 客户代表陈述，不打断，绘出脉络 具体岗位，数据规则，分支与异常，管控 绘图复述，客户代表验证，达成共识 流程派生场景观察： 典型一天， 关键时点竞品拆解头脑风暴： 意图 时间 地点 周边 功能生成： 场景-挑战-方案：场景和挑战稳定， 方案开发和需求一起讨论使用场景：事件流 UI要求 交互过程： 非必需，别忽略； 可用POP等工具； 可用界面流转图 静态快照： 直观，易于理解；可使用AxureRP；别过于追求细节 设计说明：界面元素的用途；界面的数据要求；界面的操作需求 第四节课： 其他需求管理需求明确管理需求-&gt;拆解为指标-&gt; 报表 指标完整度 数据需求数据构成；关系，范围；推演 领域模型适用场景： 1&gt;不熟悉的领域， 2&gt;大量数据复杂关系领域建模：使用母语领域建模：使用类图 类图相关说明：聚合：空心， 可独立存在组合：实心 关联存在 类图业务解读： 四色建模法 质量需求逆向思考： 危险导向的非功能需求识别关键质量需求易用性的挑战：和开发不一样的用户 POINTS 术语 的规范定义 关键业务：业务角度定义：一个流程/ 一个场景 迭代手段，增量是目标 图：是简洁表达的方式， 不简洁的图不是合格的图 人/岗位/系统/人工智能 协同办理 敏捷不是消灭分工，敏捷是专业化分工后的主动补位 实线代表工作流 虚线代表数据流 规则：容易执行 活动取代数据 大师都不用计算机工具 需求文档，架构文档，业务测试文档 对方思维流程未结束前不打断 场景： 面向大批量用户，用户态描述。独立，停止，有价值，可汇报。同比 工作职责说明书 高净值用户不care每日收益 《用户故事地图》 流程活动对应两方参与者，主/从； 一般保留服务人员的活动，用户操作手册体现用户的活动 风控；常态 时态 异态 《设计心理学》4本 体验设计 UE， UX：用户体验工程师 探索型购物需求，必须品购物需求 指标体系是数据运营的 peter coad 成就：color UML四色建模法， FDD 特征驱动开发过程 数据架构的稳定性，接受场景的挑战 春秋航空自己的订票系统，中航信 298033@qq.com OTH POINTS 需求来源： 7宗罪， 欲望 存储需求：喜欢的or不喜欢的 or 不care","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"项目","slug":"项目","permalink":"http://www.sillystone.info/tags/项目/"},{"name":"需求","slug":"需求","permalink":"http://www.sillystone.info/tags/需求/"}]},{"title":"一道考题","date":"2017-06-11T14:34:13.000Z","path":"2017/06/11/一道考题/","text":"beginning单位组织编程比赛，在线编写代码，评比指标：性能题目包括： 大量数据 文件 数据高效处理 实时界面交互 环境： 百度云，centos prepareday 1主流大数据 框架以上内容都基于hadoop， storm还依赖zookeeper 暂时忽略。 storm example spark start netty java 并发[java 并发介绍] (http://www.importnew.com/14506.html) oracle example示例程序 java并行框架 akka introduce 要点介绍 nodejs 框架适合IO高并发， 对于CPU 并行计算。。。 erlang待了解 addsbenchmark toolwrk - a HTTP benchmarking tool","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.sillystone.info/tags/java/"},{"name":"大数据","slug":"大数据","permalink":"http://www.sillystone.info/tags/大数据/"},{"name":"分布式","slug":"分布式","permalink":"http://www.sillystone.info/tags/分布式/"},{"name":"并行计算","slug":"并行计算","permalink":"http://www.sillystone.info/tags/并行计算/"}]},{"title":"db2 工具使用总结","date":"2017-05-13T01:45:30.000Z","path":"2017/05/13/db2-工具使用总结/","text":"常用工具包括：db2top db2pd, db2load db2import以下详细说明每个工具使用总结 性能工具备份/恢复import包含自增列的数据导入 数据中不包含自增列 12db2 import from import.del of del modified by identitymissing replace into table1 数据中包含自增列 12db2 import from import.del of del modified by identityignore replace into table1 loadload 后导致表空间状态为BackupPending， 以下为解释。 Normal The Normal state is the initial state of a table space after it is created, indicating that no (abnormal) states currently affect it. Load in Progress The Load in Progress state indicates that there is a load in progress on the table space. This state prevents the backup of dependent tables during the load. The table space state is distinct from the Load in Progress table state (which is used in all load operations) because the load utility places table spaces in the Load in Progress state only when you specify the COPY NO parameter for a recoverable database. The table spaces remain in this state for the duration of the load operation. * Backup Pending * If you perform a load operation for a recoverable database and specify the COPY NO parameter, table spaces are placed in the Backup Pending table space state after the first commit. You cannot update a table space in the Backup Pending state. You can remove the table space from the Backup Pending state only by backing up the table space. Even if you cancel the load operation, the table space remains in the Backup Pending state because the table space state is changed at the beginning of the load operation and cannot be rolled back. * Restore Pending * If you perform a successful load operation with the COPY NO option, restore the database, and then rollforward through that operation, the associated table spaces are placed in the Restore Pending state. To remove the table spaces from the Restore Pending state, you must perform a restore operation. 解决方法： norecoverable or copy yes12--db2 load from staff.del of del insert into staff nonrecoverable db2 load from staff.del of del insert into staff copy yes to . --推荐使用yes 123456db2set DB2_LOAD_COPY_NO_OVERRIDE=\"COPY YES TO E:\\TEST\" --db2set DB2_LOAD_COPY_NO_OVERRIDE=NONRECOVERABLE db2 terminatedb2set --显示配置结果db2 load from staff.del of del insert into staff load的 copy参数相关说明如下： COPY NO Specifies that the table space in which the table resides will be placed in backup pending state if forward recovery is enabled (that is, if either logarchmeth1 or logarchmeth2 is set to a value other than OFF). The COPY NO option will also put the table space state into the Load in Progress table space state. This is a transient state that will disappear when the load completes or aborts. The data in any table in the table space cannot be updated or deleted until a table space backup or a full database backup is made. However, it is possible to access the data in any table by using the SELECT statement. LOAD with COPY NO on a recoverable database leaves the table spaces in a backup pending state. For example, performing a LOAD with COPY NO and INDEXING MODE DEFERRED will leave indexes needing a refresh. Certain queries on the table might require an index scan and will not succeed until the indexes are refreshed. The index cannot be refreshed if it resides in a table space which is in the backup pending state. In that case, access to the table will not be allowed until a backup is taken. Index refresh is done automatically by the database when the index is accessed by a query. If one of COPY NO, COPY YES, or NONRECOVERABLE is not specified, and the database is recoverable (logarchmeth1 or logarchmeth2 is set to value other than OFF), then COPY NO is the default. COPY YES Saves a copy of the loaded data. This parameter is invalid if forward recovery is disabled. USE TSM Specifies that the copy will be stored using IBM® Tivoli® Storage Manager. OPEN num-sess SESSIONS The number of I/O sessions to be used with TSM or the vendor product. The default value is 1. TO device/directory Specifies the device or directory on which the copy image will be created. LOAD lib-name The name of the shared library (DLL on Windows operating systems) containing the vendor backup and restore I/O functions to be used. It can contain the full path. If the full path is not given, it will default to the path where the user exit programs reside. NONRECOVERABLE Specifies that the load transaction is to be marked as unrecoverable and that it will not be possible to recover it by a subsequent roll forward action. The roll forward utility will skip the transaction and will mark the table into which data was being loaded as &quot;invalid&quot;. The utility will also ignore any subsequent transactions against that table. After the roll forward operation is completed, such a table can only be dropped or restored from a backup (full or table space) taken after a commit point following the completion of the nonrecoverable load operation. With this option, table spaces are not put in backup pending state following the load operation, and a copy of the loaded data does not have to be made during the load operation. If one of COPY NO, COPY YES, or NONRECOVERABLE is not specified, and the database is not recoverable (logarchmeth1 and logarchmeth2 are both set to OFF), then NONRECOVERABLE is the default. db2look 导出表结构db2look -d db_name -e -o output 导出数据表，权限，表空间和缓冲，db2look -d db_name -e -a -l -x -o output","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"db2","slug":"db2","permalink":"http://www.sillystone.info/tags/db2/"}]},{"title":"在路上-台湾","date":"2017-05-12T14:15:00.000Z","path":"2017/05/12/宝岛点滴/","text":"台湾记忆街道台北的早上很安静， 干净的街道没什么人，商场，西餐店，咖啡店都不营业。 走了很久才找到一个早餐店，店里主要是卖蛋饼，往来的都是早起的大叔，或是出门买早餐的小孩子，吃早餐的人不少但不合我的口味。 台铁月台，传说中的月台， 可以标识出车厢，好神奇，为什么国内的站台不行花莲-瑞芳，倒换了三张票，也是醉了台铁好便宜。。。 地铁 - 捷运可能是周末的原因，地铁里人很少，座位都是空的，相比北京的人挤人，舒适多了。 我搭乘最多的是绿线（松山-新店）和红线（淡水-象山），从南京复兴到七分，大约40分钟，一路没什么人，大家都很安静，周末早上出行的人都为生活奔波着，没什么生气。 爱心座经常是空着的，我只好站着 公交最让我感到新奇的交通工具，上车要招手，否则可能飞驰而过，下车要提前按铃，公交上到处都可以很方便的找到下车按钮，车上人也不多，最好坐在座位上，座位上有安全带，可能因为车开的快有些路段又有山路的原因吧。 最后就是车上的刷卡指示牌， 我估计公交是分段计价的，一段路程是上车刷卡，一段路程下车刷卡，路程长的话可能上下都要刷卡，总之按车上的指示牌刷卡就好台北的司机 下车都要说声谢谢，感觉好客气 摩托台北的摩托车开的好快，绿灯刚一亮就飞了出去，路上总能听到呼啸而过的摩托车，面对飞车，我只好规规矩矩的等红绿灯有机会还是希望骑一下摩托车，感受一下风声。 电动车那辆 hello kitty电动车，追风的少年，迷人的海景 台湾人公交司机下车会说谢谢火车站的服务人员耐心的解说一日游巴士司机热心的拍照美术馆服务人员详细的解说宾馆，民宿周到的服务 显示出台湾人的服务； 国父纪念馆外练习街舞的学生 零碎浦发AE白，你值得拥有，谁用谁知道","categories":[{"name":"随笔","slug":"随笔","permalink":"http://www.sillystone.info/categories/随笔/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"http://www.sillystone.info/tags/随笔/"}]},{"title":"台湾","date":"2017-05-05T11:55:47.000Z","path":"2017/05/05/台湾/","text":"宝岛9日游, 多图预警 台北 故宫里的宝贝 高雄 中山大学 垦丁","categories":[{"name":"摄影","slug":"摄影","permalink":"http://www.sillystone.info/categories/摄影/"}],"tags":[{"name":"日记，摄影","slug":"日记，摄影","permalink":"http://www.sillystone.info/tags/日记，摄影/"}]},{"title":"山本耀司-文摘","date":"2017-04-09T09:47:11.000Z","path":"2017/04/09/山本耀司-文摘/","text":"山本耀司 日文名Yohji Yamamoto 走上旅程，我兜里有钱 买梦，我兜里有钱 我也能为她买一段生活 但为什么，我的背脊感到一丝凉意 奇，奇怪 奔跑的一生，不停歇 到达的终点，总会有 没什么能让我悲伤 只是背脊上那一丝凉意 * 过分认真的为那一天而活 我不能为那一天而活 好，我要出发。时间已到。 窗外司空见惯的风景 却好久不曾见到 玻璃窗外，布满污点和灰尘，同我一样 喜爱的绿色被黑色覆盖 新发的嫩芽，散发黄绿的亮丽色彩 好像我有点太过慵懒 好像我有点过于担忧 别发牢骚，会被人耻笑 要挺直腰杆做人 为那一天而活的意义在于 没办法为那一天而活 好，我要出发。时间已到 对你来说，贫困潦倒时用最后几个硬币买来的啤酒的味道，与，在半岛酒店的房间里穿着柔软 的浴袍喝着的冰香槟的味道，并无差别 今天，太阳依旧升起 当然，我无意揭露你明知故犯，违背自己原则的那一面 你，就是你 而我，就是喜欢你。 “自己”这个东西是看不见的，撞上一些别的什么,反弹回来，才会了解“自己”。 所以，跟很强的东西、可怕的东西、水准很高的东西相碰撞，然后才知道“自己”是什么，这才是自我 “时尚”不会让你变的性感，你的“经历”和“想象力”才能让你变得性感。 而要得到这些性感没有捷径，唯一的方法就是你得好好生活。 而我的内心总有一小部分，保留了一个顽皮捣蛋的孩童的内心。他只会突然转过身来， 对整个世界放肆地吐吐舌头。这样的我，面对他人的赞许，会不自在起来","categories":[{"name":"随笔","slug":"随笔","permalink":"http://www.sillystone.info/categories/随笔/"}],"tags":[{"name":"诗","slug":"诗","permalink":"http://www.sillystone.info/tags/诗/"},{"name":"文摘","slug":"文摘","permalink":"http://www.sillystone.info/tags/文摘/"}]},{"title":"养鱼记-大鱼吃小鱼","date":"2017-04-08T12:48:46.000Z","path":"2017/04/08/养鱼记-大鱼吃小鱼/","text":"早上突然发现鱼缸里有一条小鱼 黑色的，小小的 蝌蚪一样 他 应该刚出生不久 从草丛中游了出来 然后 一只燕鱼出现了 他，消失了 短暂的生命 多彩的世界 瞬间变成黑暗 后记：只发现两条小鱼，发现的瞬间，就被吃了，估计其他的小鱼也是如此","categories":[{"name":"随笔","slug":"随笔","permalink":"http://www.sillystone.info/categories/随笔/"}],"tags":[{"name":"日记","slug":"日记","permalink":"http://www.sillystone.info/tags/日记/"}]},{"title":"前端之CSS","date":"2017-03-30T14:29:49.000Z","path":"2017/03/30/前端之CSS/","text":"前端组件1. Dialogbootstrap-dialog满足所有Dialog的需求 修订需求： 全屏对话框：用来实现大量内容加载和录入 问题：对话框加载的页面如果是个组合页面（即:页面套页面），对话框大小无法自适应高度 修订：对于dialog的message内容定制 高度属性/css 类 参见：bootstrap-dialog 2. Table待补充","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"http://www.sillystone.info/tags/javascript/"},{"name":"css","slug":"css","permalink":"http://www.sillystone.info/tags/css/"}]},{"title":"Linux相关","date":"2017-03-29T09:14:32.000Z","path":"2017/03/29/Linux相关/","text":"Linux 相关总结 系统管理系统资源进程资源目录：/proc/进程ID/可查看某个进程的相关信息， 其中limits文件中包含该进程相关信息，示例如下：12345678Limit Soft Limit ... Max cpu time unlimited Max file size unlimited Max data size unlimited Max stack size 10485760 Max core file size unlimited Max processes unlimited Max open files 8192 工具sed12# 从建表ddl中筛选所有表名，转大写grep -i \"create table\" create_table.sql|sed 's/.*\\.\\(.*\\)(/\\1/' |tr a-z A-Z awkcrontab11 23 * * 2,5 /home/user1/reorg.sh &gt;&gt; /home/user1/reorg.log 2&gt;&amp;1 jar用法: jar {ctxui}[vfm0Me] [jar-file] [manifest-file] [entry-point] [-C dir] files …选项包括： -c 创建新的归档文件 -t 列出归档目录 -x 解压缩已归档的指定（或所有）文件 -u 更新现有的归档文件 -v 在标准输出中生成详细输出 -f 指定归档文件名 -m 包含指定清单文件中的清单信息 -e 为捆绑到可执行 jar 文件的独立应用程序 指定应用程序入口点 -0 仅存储；不使用任何 ZIP 压缩 -M 不创建条目的清单文件 -i 为指定的 jar 文件生成索引信息 -C 更改为指定的目录并包含其中的文件如果有任何目录文件，则对其进行递归处理。清单文件名、归档文件名和入口点名的指定顺序与 “m”、”f” 和 “e” 标志的指定顺序相同。 12345678# 将两个类文件归档到一个名为 classes.jar 的归档文件中jar cvf classes.jar Foo.class Bar.class# 使用现有的清单文件 \"mymanifest\" 并将 foo/ 目录中的所有文件归档到 \"classes.jar\" 中jar cvfm classes.jar mymanifest -C foo/ .# 将wbe目录下所有文件 归档到 “classes.jar\"中jar cvf classes.jar -C web/ . 可以使用 vi直接编辑 jar包 * grep在Linux中要正确匹配tab（退格）符有两种方式 1：用 grep $’\\t’ 你的文件2：用 grep ‘按CTRL+V 键，再按TAB键’ 你的文件 性能监控 top第一行后面的三个值是系统在之前 1、5、15 的平均负载，也可以看出系统负载是上升、平稳、下降的趋势，当这个值超过 CPU 可执行单元的数目，则表示 CPU 的性能已经饱和成为瓶颈了。 第二行统计了系统的任务状态信息。running 很自然不必多说，包括正在 CPU 上运行的和将要被调度运行的；sleeping 通常是等待事件(比如 IO 操作)完成的任务，细分可以包括 interruptible 和 uninterruptible 的类型；stopped 是一些被暂停的任务，通常发送 SIGSTOP 或者对一个前台任务操作 Ctrl-Z 可以将其暂停；zombie 僵尸任务，虽然进程终止资源会被自动回收，但是含有退出任务的 task descriptor 需要父进程访问后才能释放，这种进程显示为 defunct 状态，无论是因为父进程提前退出还是未 wait 调用，出现这种进程都应该格外注意程序是否设计有误。 第三行 CPU 占用率根据类型有以下几种情况： (us) user：CPU 在低 nice 值(高优先级)用户态所占用的时间(nice&lt;=0)。正常情况下只要服务器不是很闲，那么大部分的 CPU 时间应该都在此执行这类程序(sy) system：CPU 处于内核态所占用的时间，操作系统通过系统调用(system call)从用户态陷入内核态，以执行特定的服务；通常情况下该值会比较小，但是当服务器执行的 IO 比较密集的时候，该值会比较大(ni) nice：CPU 在高 nice 值(低优先级)用户态以低优先级运行占用的时间(nice&gt;0)。默认新启动的进程 nice=0，是不会计入这里的，除非手动通过 renice 或者 setpriority() 的方式修改程序的nice值(id) idle：CPU 在空闲状态(执行 kernel idle handler )所占用的时间(wa) iowait：等待 IO 完成做占用的时间(hi) irq：系统处理硬件中断所消耗的时间(si) softirq：系统处理软中断所消耗的时间，记住软中断分为 softirqs、tasklets (其实是前者的特例)、work queues，不知道这里是统计的是哪些的时间，毕竟 work queues 的执行已经不是中断上下文了(st) steal：在虚拟机情况下才有意义，因为虚拟机下 CPU 也是共享物理 CPU 的，所以这段时间表明虚拟机等待 hypervisor 调度 CPU 的时间，也意味着这段时间 hypervisor 将 CPU 调度给别的 CPU 执行，这个时段的 CPU 资源被“stolen”了。这个值在我 KVM 的 VPS 机器上是不为 0 的，但也只有 0.1 这个数量级，是不是可以用来判断 VPS 超售的情况？ CPU 占用率高很多情况下意味着一些东西，这也给服务器 CPU 使用率过高情况下指明了相应地排查思路： 当 user 占用率过高的时候，通常是某些个别的进程占用了大量的 CPU，这时候很容易通过 top 找到该程序；此时如果怀疑程序异常，可以通过 perf 等思路找出热点调用函数来进一步排查；当 system 占用率过高的时候，如果 IO 操作(包括终端 IO)比较多，可能会造成这部分的 CPU 占用率高，比如在 file server、database server 等类型的服务器上，否则(比如&gt;20%)很可能有些部分的内核、驱动模块有问题；当 nice 占用率过高的时候，通常是有意行为，当进程的发起者知道某些进程占用较高的 CPU，会设置其 nice 值确保不会淹没其他进程对 CPU 的使用请求；当 iowait 占用率过高的时候，通常意味着某些程序的 IO 操作效率很低，或者 IO 对应设备的性能很低以至于读写操作需要很长的时间来完成；当 irq/softirq 占用率过高的时候，很可能某些外设出现问题，导致产生大量的irq请求，这时候通过检查 /proc/interrupts 文件来深究问题所在；当 steal 占用率过高的时候，黑心厂商虚拟机超售了吧！ 第四行和第五行是物理内存和虚拟内存(交换分区)的信息： total = free + used + buff/cache，现在buffers和cached Mem信息总和到一起了，但是buffers和cached Mem 的关系很多地方都没说清楚。其实通过对比数据，这两个值就是 /proc/meminfo 中的 Buffers 和 Cached 字段：Buffers 是针对 raw disk 的块缓存，主要是以 raw block 的方式缓存文件系统的元数据(比如超级块信息等)，这个值一般比较小(20M左右)；而 Cached 是针对于某些具体的文件进行读缓存，以增加文件的访问效率而使用的，可以说是用于文件系统中文件缓存使用。 而 avail Mem 是一个新的参数值，用于指示在不进行交换的情况下，可以给新开启的程序多少内存空间，大致和 free + buff/cached 相当，而这也印证了上面的说法，free + buffers + cached Mem才是真正可用的物理内存。并且，使用交换分区不见得是坏事情，所以交换分区使用率不是什么严重的参数，但是频繁的 swap in/out 就不是好事情了，这种情况需要注意，通常表示物理内存紧缺的情况。 最后是每个程序的资源占用列表，其中 CPU 的使用率是所有 CPU core 占用率的总和。通常执行 top 的时候，本身该程序会大量的读取 /proc 操作，所以基本该 top 程序本身也会是名列前茅的。 top 虽然非常强大，但是通常用于控制台实时监测系统信息，不适合长时间(几天、几个月)监测系统的负载信息，同时对于短命的进程也会遗漏无法给出统计信息 vmstatr 表示可运行进程数目，数据大致相符；而b表示的是 uninterruptible 睡眠的进程数目；swpd 表示使用到的虚拟内存数量，跟 top-Swap-used 的数值是一个含义，而如手册所说，通常情况下 buffers 数目要比 cached Mem 小的多，buffers 一般20M这么个数量级；io 域的 bi、bo 表明每秒钟向磁盘接收和发送的块数目(blocks/s)；system 域的 in 表明每秒钟的系统中断数(包括时钟中断)，cs表明因为进程切换导致上下文切换的数目。 说到这里，想到以前很多人纠结编译 linux kernel 的时候 -j 参数究竟是 CPU Core 还是 CPU Core+1？通过上面修改 -j 参数值编译 boost 和 linux kernel 的同时开启 vmstat 监控，发现两种情况下 context switch 基本没有变化，且也只有显著增加 -j 值后 context switch 才会有显著的增加，看来不必过于纠结这个参数了，虽然具体编译时间长度我还没有测试。资料说如果不是在系统启动或者 benchmark 的状态，参数 context switch&gt;100000 程序肯定有问题 pidstat如果想对某个进程进行全面具体的追踪，没有什么比 pidstat 更合适的了——栈空间、缺页情况、主被动切换等信息尽收眼底。这个命令最有用的参数是-t，可以将进程中各个线程的详细信息罗列出来。 -r： 显示缺页错误和内存使用状况，缺页错误是程序需要访问映射在虚拟内存空间中但是还尚未被加载到物理内存中的一个分页，缺页错误两个主要类型是 minflt/s 指的 minor faults，当需要访问的物理页面因为某些原因(比如共享页面、缓存机制等)已经存在于物理内存中了，只是在当前进程的页表中没有引用，MMU 只需要设置对应的 entry 就可以了，这个代价是相当小的majflt/s 指的 major faults，MMU 需要在当前可用物理内存中申请一块空闲的物理页面(如果没有可用的空闲页面，则需要将别的物理页面切换到交换空间去以释放得到空闲物理页面)，然后从外部加载数据到该物理页面中，并设置好对应的 entry，这个代价是相当高的，和前者有几个数据级的差异 -s：栈使用状况，包括 StkSize 为线程保留的栈空间，以及 StkRef 实际使用的栈空间。使用ulimit -s发现CentOS 6.x上面默认栈空间是10240K，而 CentOS 7.x、Ubuntu系列默认栈空间大小为8196K -u：CPU使用率情况，参数同前面类似 -w：线程上下文切换的数目，还细分为cswch/s因为等待资源等因素导致的主动切换，以及nvcswch/s线程CPU时间导致的被动切换的统计 如果每次都先ps得到程序的pid后再操作pidstat会显得很麻烦，所以这个杀手锏的-C可以指定某个字符串，然后Command中如果包含这个字符串，那么该程序的信息就会被打印统计出来，-l可以显示完整的程序名和参数 ➜ ~ pidstat -w -t -C “ailaw” -l 其他➜ ~ mpstat -P ALL 1➜ ~ ps axjf IOiotop 可以直观的显示各个进程、线程的磁盘读取实时速率；lsof 不仅可以显示普通文件的打开信息(使用者)，还可以操作 /dev/sda1 这类设备文件的打开信息，那么比如当分区无法 umount 的时候，就可以通过 lsof 找出磁盘该分区的使用状态了，而且添加 +fg 参数还可以额外显示文件打开 flag 标记 iostat➜ ~ iostat -xz 1 其实无论使用 iostat -xz 1 还是使用 sar -d 1，对于磁盘重要的参数是： avgqu-s：发送给设备 I/O 请求的等待队列平均长度，对于单个磁盘如果值&gt;1表明设备饱和，对于多个磁盘阵列的逻辑磁盘情况除外 await(r_await、w_await)：平均每次设备 I/O 请求操作的等待时间(ms)，包含请求排列在队列中和被服务的时间之和； svctm：发送给设备 I/O 请求的平均服务时间(ms)，如果 svctm 与 await 很接近，表示几乎没有 I/O 等待，磁盘性能很好，否则磁盘队列等待时间较长，磁盘响应较差； %util：设备的使用率，表明每秒中用于 I/O 工作时间的占比，单个磁盘当 %util&gt;60% 的时候性能就会下降(体现在 await 也会增加)，当接近100%时候就设备饱和了，但对于有多个磁盘阵列的逻辑磁盘情况除外； 还有，虽然监测到的磁盘性能比较差，但是不一定会对应用程序的响应造成影响，内核通常使用 I/O asynchronously 技术，使用读写缓存技术来改善性能，不过这又跟上面的物理内存的限制相制约了。 上面的这些参数，对网络文件系统也是受用的。 netstat➜ ~ netstat -s 显示自从系统启动以来，各个协议的总体数据信息。虽然参数信息比较丰富有用，但是累计值，除非两次运行做差才能得出当前系统的网络状态信息，亦或者使用 watch 眼睛直观其数值变化趋势。所以netstat通常用来检测端口和连接信息的： netstat –all(a) –numeric(n) –tcp(t) –udp(u) –timers(o) –listening(l) –program(p) –timers可以取消域名反向查询，加快显示速度；比较常用的有 ➜ ~ netstat -antp #列出所有TCP的连接➜ ~ netstat -nltp #列出本地所有TCP侦听套接字，不要加-a参数 sarsar 这个工具太强大了，什么 CPU、磁盘、页面交换啥都管，这里使用 -n 主要用来分析网络活动，虽然网络中它还给细分了 NFS、IP、ICMP、SOCK 等各种层次各种协议的数据信息，我们只关心 TCP 和 UDP。下面的命令除了显示常规情况下段、数据报的收发情况，还包括 TCP ➜ ~ sudo sar -n TCP,ETCP 1 active/s：本地发起的 TCP 连接，比如通过 connect()，TCP 的状态从CLOSED -&gt; SYN-SENT passive/s：由远程发起的 TCP 连接，比如通过 accept()，TCP 的状态从LISTEN -&gt; SYN-RCVD retrans/s(tcpRetransSegs)：每秒钟 TCP 重传数目，通常在网络质量差，或者服务器过载后丢包的情况下，根据 TCP 的确认重传机制会发生重传操作 isegerr/s(tcpInErrs)：每秒钟接收到出错的数据包(比如 checksum 失败) UDP ➜ ~ sudo sar -n UDP 1 noport/s(udpNoPorts)：每秒钟接收到的但是却没有应用程序在指定目的端口的数据报个数 idgmerr/s(udpInErrors)：除了上面原因之外的本机接收到但却无法派发的数据报个数 当然，这些数据一定程度上可以说明网络可靠性，但也只有同具体的业务需求场景结合起来才具有意义。 tcpdump这么看来，如果查看单个尤其是多线程的任务时候，pidstat比常用的ps更好使","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.sillystone.info/tags/Linux/"}]},{"title":"倒立","date":"2017-03-28T16:29:20.000Z","path":"2017/03/29/倒立/","text":"尝试倒立成功","categories":[{"name":"随笔","slug":"随笔","permalink":"http://www.sillystone.info/categories/随笔/"}],"tags":[{"name":"日记","slug":"日记","permalink":"http://www.sillystone.info/tags/日记/"}]},{"title":"批量设计","date":"2017-03-28T02:04:14.000Z","path":"2017/03/28/批量设计/","text":"概述批量作业相对于联机交易而言，主要用于实现数据加工（计算，统计） 一般批量作业的特点 大量数据筛选，加工。消耗数据库资源：CPU/IO 利用数据库存储过程，数据处理在数据库上执行数据加工过程使用临时表大量数据处理，使用游标分批加工，减少日志空间的消耗，和错误引起执行效率底下问题数据导入导出使用数据库工具（DB2：load/import） 数据库上执行，存在单点问题 加工过程不易输出异常信息 特定需求： 7*24小时服务，系统不停机 （数据库资源不能消耗太高）全球服务：业务数据隔离， 批处理调度作业按时区划分并行处理设计（基于应用服务器） 解决思路 批量需求新技术和新技术方案越来越多，可以满足各类业务需求，对于批量需求首要考虑其必要性应当尽量减少批量作业数量，联机方案替代批量作业 应用服务器并行作业处理","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"批量","slug":"批量","permalink":"http://www.sillystone.info/tags/批量/"}]},{"title":"鸡毛飞上天","date":"2017-03-25T06:57:16.000Z","path":"2017/03/25/鸡毛飞上天/","text":"天冷 裹被子看剧 义乌 是个好地方 感情是 经历过的苦难熬出的糖","categories":[{"name":"随笔","slug":"随笔","permalink":"http://www.sillystone.info/categories/随笔/"}],"tags":[{"name":"日记","slug":"日记","permalink":"http://www.sillystone.info/tags/日记/"},{"name":"电视剧","slug":"电视剧","permalink":"http://www.sillystone.info/tags/电视剧/"}]},{"title":"新职业-视频剪辑师","date":"2017-03-25T04:31:20.000Z","path":"2017/03/25/新职业-视频剪辑师/","text":"最近负责制作一段demo演示视频，用来给业务展示新建系统的UI界面和功能。视频剪辑是个体力活—-脖子疼 录屏软件google后，试用了几个 camstudio 历史悠久，广泛使用，大约13M window系统需要安装 对应版本的 visual c++ redistributable package（5M）功能简单 shareX 开源软件 大约5M 录屏需要安装ffmpeg（32位或者64位）功能简单，界面友好 screenpresso 大约13M 录屏也需要ffmpeg 可能需安装.net framework work 对应版本 ActivePresenter 35M 可录屏，可编辑制作视频 最终录屏使用的 shareX（ffmpeg) 安装文件小开源软件使用简单UI很好 视频编辑制作列表如下： 国内软件：会声会影 名气很大，安装文件太大 没有安装 国内软件：爱剪辑 283M avidemux： 20M 没有找到视频剪辑功能 ActivePresenter： 最终使用 ActivePresenter： 可以剪切视频 放大/缩小 添加备注 增加：图片，PPT等 制作的demo","categories":[{"name":"随笔","slug":"随笔","permalink":"http://www.sillystone.info/categories/随笔/"}],"tags":[{"name":"日记","slug":"日记","permalink":"http://www.sillystone.info/tags/日记/"}]},{"title":"贷前调查","date":"2017-03-23T14:45:42.000Z","path":"2017/03/23/贷前调查/","text":"来源：微信公众号 信贷共学 一、行业1、什么行业？2、行业所处阶段？（新兴、成熟、衰退）3、行业是否受以经济周期影响？4、行业的整体盈利情况？与企业盈利状况比较。5、企业规模及市场占有率状况，是否为龙头企业？6、行业产能是否过剩？7、行业经营资质是否需有关部门审批？是否受法律政策限制或影响？ 二、股东和经营者8、企业性质是什么？（民营、国有、股份、合资、独资、合伙、个体）9、股权结构是什么？谁控股？企业实际控制人是谁？10、股东以什么方式投资？是否到位？是否抽逃？11、是否有经营团队？经营团队是否有足够的经验？12、企业的组织构架是什么？管理是否有深度和广度？管理是否规范？企业的激励机制和用人机制是什么？企业文化是什么？13、是否有董事会？是否有独立董事？董事会是否在行使职权？董事会对管理层是否有足够的制约？14、企业的历史沿革，近三年经营班子的目标是什么？是否完成？（年终总结）15、企业的母公司、子公司、兄弟公司及其他关联企业有多少？股东的关联企业有多少？关联企业在社会的信誉如何？在我行和他行的融资情况如何？16、企业的关联企业与企业采购合同和购销合同的价格、数量、结算是否真实？是否符合市场行情？ 三、生产经营17、生产什么产品或提供什么服务？18、产品或服务的特点是什么？竞争力是什么？（差异优势、技术优势、成本优势、关系优势）19、是否有替代品将影响企业经营？20、产品的市场供求状况分析，是供大于求，还是供不应求？21、客户对企业产品的依赖度如何？（单一产品）22、原材料价格波动趋势和供给情况如何，对公司产品销售的影响是什么？23、原材料质量、保存有什么特殊要求？24、生产是否符合环保要求？（环保批文）25、生产技术是否落后？是否具有自主知识产权？与同行业比较？26、产品的销售渠道是什么？27、是否有销售团队？销售手段是什么？28、产品的销售政策是什么？是否赊销？是否有现金折扣？ 四、财务29、企业近三年的销售收入是多少？是增长、下降还是波动？原因是什么？30、企业近三年的利润总额和销售利润率是多少？是增长、下降还是波动？原因是什么？31、企业近三年的资产、负债、所有者权益是多少？资产负负债率是否符合行业特征？32、企业近三年的经营性现金净流量是多少？现金净流量与利润总额之间的差额是多少？现金净流量小于利润总额的原因是什么？33、企业流动资产和流动负债是多少？流动负债与流动资产的差额是多少？流动负债过高是否有短贷长用的现象？34、货币资金中保证金存款和已冻结的定期存单存款有多少？货币资金与抽查的现金日记账、银行日记账、银行对账单是否相符？35、应收账款明细是什么？一年以上账龄的应收账款有多少？应收账款坏账有多少？是否按财务制度规定计提准备？36、应收账款客户是否集中、单一？单一客户是否可能违约或取消合作？37、应收账款周转速度是否符合同业标准？38、应收账款增长额是多少？应收账款增长额与销售收入增长额之间的关系？与利润总额之间的关系？39、存货明细是什么？存货结构是什么？40、一年以上产成品、半成品有多少？41、原材料的采购成本与市场价格之间的差额是多少？是否计提存货跌价损失？42、存货周转速度是否符合同业标准？43、存货增长额是多少？存货增长额与销售收入增长额之间的关系？与固定资产增长额之间的关系？与利润总额之间的关系？44、存货的结转方法是什么？对当期成本和利润的影响是什么？45、存货存放在场内还是场外？存货是否已在他行或第三人质押？46、预付账款明细？采购价格与市场价格比较？是否符合采购合同约定？47、其他应收款明细是什么？在同业中占比是否过高？有无与实收资本金额相近的大额其他应收账款？有无注册资本抽逃现象？有无大额资产转移现象？48、固定资产明细是什么？产权是否明晰？49、固定资产入账依据是什么？固定资产账面价值与实际购买价值是否相符？固定资产评估入账的依据是否充分？评估价值是否过高？50、固定资产是否已在他行或第三人抵押？51、固定资产折旧方法是否符合财务制度规定？对当期的利润影响是什么？52、设备是否为专业设备？是否已被淘汰？变现能力如何？53、在建工程的总投资多少？已投资多少？竣工验收还需投资多少？在建工程投入使用对未来销售收入、利润、融资需求的影响？在建工程是否已抵押？54、土地性质是什么？是否已缴纳全额土地出让金？是否先征后返？地方是否有禁止转让或补交土地出让金后转让的要求？土地是否已抵押？55、固定资产是否已出租？出租合同的期限和付款方式是什么？出租价格是否合理？承租人是否同意租赁人违约时解除租赁合同或将租金缴纳给银行？56、短期借款、长期借款、应付票据明细？各银行授信金额多少？授信余额多少？到期日？利率水平？是否逾期？五级分类？担保方式？是否有短贷长用现象？57、应付账款明细？应付账款期限是多少？一年以上应付账款有多少？是否已违约？是否有纠纷？58、应付账款增长额是多少？是否高于往年增长额？是否高于平时增长额？是否有调整经营性现金净流量的嫌疑？59、预收账款明细？预收账款金额与合同约定生产进度是否相符？是否已开出预收账款保函？60、企业营业税缴纳多少？企业进项增值税多少？销项增值税多少？是否与税单相符？与报表销售收入是否匹配？61、企业是否享受各种税费减免政策？是否享受各种补贴？62、企业实收资本多少？注资方式是什么？是否抽逃？63、企业资本公积计账是否合理？固定资产评估增值是否符合市场价格？64、企业经营性现金流入量占销售收入比是多少？65、销售成本占比是否异常？是否存在少结转成本增加利润现象？66、投资收益率是否符合同业水平？投资收益中获得现金的比率是否正常？长期投资是否存在不良资产？ 五、用途、期限和还款来源67、企业的贷款用途是什么？68、是否有订单？69、项目贷款是否符合手续？如立项、可研、环评？70、自有资金有多少？占资金需求比例是多少？是否已到位？71、贷款的还款来源是什么？测算还款来源是否扣除铺底流动资金？72、何时还款？流动资金贷款测算企业的经营周期，项目贷款测算企业的每年净现金流量。 六、风险控制手段73、如何控制企业信贷资金流出和销售资金回笼？74、如何监管企业生产或项目进度？75、担保方式是什么？76、抵押物评估价值是否偏高？抵押物是否有瑕疵？77、保证人还款能力如何？78、仓单如何监管？79、是否同意签订个人无限责任担保和远期拍卖协议？ 七、收益测算80、授信产品如何设计？81、是否在我行开立基本账户？82、贷款利率是否符合企业信用等级标准？83、是否有中间业务收入？84、企业是否在我行代发工资？85、法定代表人是否办理贵宾卡？86、员工是否办理信用卡？87、关联企业是否办理网银？88、关联企业和上下游企业是否在我行开户？下游企业是否办理保理？上游企业可否办理保兑仓业务？89、授信余额是否超我行集中度要求？90、信用等级是否在AA级以上？91、抵押率是否在50%以下？92、授信业务种类和行业收取的资本占用费率是否最低？ 八、企业的社会信誉93、是否到工商局查询企业年检状况、公司章程和股东变更记录？94、是否到房交所查询企业固定资产是否已抵押或查封？95、是否到房交所查询企业法定代表人个人财产状况？96、是否到其他融资银行了解企业的信誉状况？97、是否到同业或行业协会了解企业在同业间的口碑？ 九、法律要件98、是否有董事会决议？99、借款合同、担保合同是否当面签章？100、个人无限责任夫妻双方是否当面签字？","categories":[{"name":"金融","slug":"金融","permalink":"http://www.sillystone.info/categories/金融/"}],"tags":[{"name":"银行","slug":"银行","permalink":"http://www.sillystone.info/tags/银行/"},{"name":"信贷","slug":"信贷","permalink":"http://www.sillystone.info/tags/信贷/"}]},{"title":"第一次","date":"2017-03-21T14:00:17.000Z","path":"2017/03/21/第一次/","text":"看了一眼央视的 《朗读者》喜欢读书，思想的沉淀，喜欢这期节目开头的 《第一次》 当你呱呱落地的那一瞬间, 你开始了在这个人世间第一次的神奇之旅， 第一次拉开了你人生的帷幕， 随后， 你开始经历各种各样形形色色的第一次： 第一次哭，第一次笑，第一次走路，第一次成功，第一次失败， 它是生命的体验， 它是成长的过程， 因为不会再重来， 所以难忘， 所以让人珍惜 第一次往往需要勇气， 但是第一次也往往会有意想不到的收获， 因为它是探索， 是挑战， 是机遇。 如果人生拥有越多的第一次， 也意味着人生越丰富 生活是个过程","categories":[{"name":"随笔","slug":"随笔","permalink":"http://www.sillystone.info/categories/随笔/"}],"tags":[{"name":"日记","slug":"日记","permalink":"http://www.sillystone.info/tags/日记/"}]},{"title":"架构决策","date":"2017-03-21T13:59:52.000Z","path":"2017/03/21/架构决策/","text":"架构决策 面向未来，明确目标架构 分析现状，确定实施线路 解决问题，解决必要需求，放弃次要功能 避免频繁修订，妥协，偏离目标。","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[]},{"title":"web服务器 调试","date":"2017-03-21T13:59:36.000Z","path":"2017/03/21/jetty-调试/","text":"项目实践过的各种服务器调试总结，包括：IBM WAS， jetty 场景 远程实时调试web server：一般用于分析某个事件出发的系统处理异常，需单步跟踪 分析java web server运行状态：包括内存溢出，cpu 100% 实践远程实时调试服务器IBM websphere 服务器设置： 开启服务器调试端口重启web server 本地设置 Run-&gt;Debug configuration新建 Remote jetty 本地设置（基于IntelliJ IDEA）： Run-&gt;Debug configuration新建 Remote配置服务器 HOST 和 PORT ; 端口可设置1000以上非常用端口复制服务器命令行参数： Command line arguments for running remote JVM 1agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 服务器设置： jetty启动时增加 命令参数（上文复制的内容）示例如下： 1java -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 -jar &#123;jetty path&#125; &#123;jetty port&#125; --path &#123;your war&#125; 1&gt;/dev/null 2&gt;&amp;1 &amp; 设置本地代码断点，启动remote server 触发服务器事件，在IDE中跟踪断点 分析java web server运行状态java自带工具jpsjps主要用来输出JVM中运行的进程状态信息 jstack用来查看某个Java进程内的线程堆栈信息1234567jstack [option] pidjstack [option] executable corejstack [option] [server-id@]remote-hostname-or-ip# option-l long listings，会打印出额外的锁信息，在发生死锁时可以用jstack -l pid来观察锁持有情况-m mixed mode，不仅会输出Java堆栈信息，还会输出C/C++堆栈信息（比如Native方法） 使用示例123456789# 获取进程PIDps -ef | grep JavaServiceName # 查看线程 ps -Lfp pid or ps -mp pid -o THREAD,tid,time# time列 是线程运行时间， 获取线程idtop -Hp pid# 线程id转为 16进制echo &quot;obase=16;ibase=10;23187&quot; |bc # 使用jstack&#123;java home&#125;jstack pid | grep 43A1 #16进制结果 jmap jhatjmap用于查看堆内存，结合jhat使用123456789# 查看堆&#123;java home&#125;/jmap -heap pid# 使用jmap -histo[:live] pid查看堆内存中的对象数目、大小统计直方图 jmap -histo:live pid | more# 输出到dump文件&#123;java home&#125;/jmap -dump:format=b,file=/tmp/dump.dat 5961# dump 信息使用jhat查看&#123;java home&#125;/jhat -J-Xmx512m /tmp/dump.data visualVm可用于远程分析 jstat123jstat [ generalOption | outputOptions vmid [interval[s|ms] [count]] ]# GC信息，采样时间间隔为250ms，采样数为4jstat -gc 21711 250 4 hprof展现CPU使用率，统计堆内存123456789101112java -agentlib:hprof[=options] ToBeProfiledClassjava -Xrunprof[:options] ToBeProfiledClassjavac -J-agentlib:hprof[=options] ToBeProfiledClass# 每隔20毫秒采样CPU消耗信息，堆栈深度为3，生成的profile文件名称是java.hprof.txtjava -agentlib:hprof=cpu=samples,interval=20,depth=3 Hellojavac -J-agentlib:hprof=cpu=times Hello.javajavac -J-agentlib:hprof=heap=sites Hello.javajavac -J-agentlib:hprof=heap=dump Hello.java IBM websphere 生成分析日志 was运行异常默认在profile目录生成javacore snapshot heapdump 信息linux下 使用 kill命令 触发was server 生成javacore 1kill -3 PID 使用工具分析日志IBM官网提供了工具jcaNNN.jar (备注: NNN版本号) 1&lt;Java Runtime Environment path&gt;java -Xmx500m -jar jca457.jar 需要根据heapdump大小 调整Xmx参数值，实操过程中2G heapdump，可能需要&gt;4G内存。远程服务器可以通过xshell工具（支持远程UI，需远程服务器支持图形相关库）调试。 jetty 9 服务器设置jetty官网说明 启用JMX 命令示例如下，执行后 会在jetty.base目录的start.ini中添加 jmx相关参数。启动服务 $ java -jar {$jetty.home}/start.jar 1java -jar /usr/local/jett9/bin/start.jar --add-to-start=jmx-remote jmx相关参数：123456--module=jmxjetty.jmxrmihost=localhostjetty.jmxrmiport=1099-Dcom.sun.management.jmxremote-Dcom.sun.management.jmxremote.ssl=false-Dcom.sun.management.jmxremote.authenticate=false jmx配置： {jetty home}/etc/jetty-jmx.xml 工具使用使用jconsoleoracle说明1jconsole -J-DsocksProxyHost=localhost -J-DsocksProxyPort=1099 RemoteServer:1099 visualVm 执行 visualVm新建remote ，配置远程服务器的地址 后注问题一：was dump问题 heap空间不足 导致dump，可以用IBM工具分析 系统资源不足报错信息：java.lang.OutOfMemoryError: Failed to create a thread: retVal -1073741830, errno 11系统资源不足，无法创建线程。 javacore：122CIUSERLIMIT RLIMIT_NOFILE 1024 655362CIUSERLIMIT RLIMIT_NPROC 1024 20480 解决：修改 /etc/security/limits.d/90-nproc.conf1* soft nproc 10240 需要重新启动server nodeagent 可以参考查看Linux进程资源来确定进程的limits参数是否修改成功 3.","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"jetty","slug":"jetty","permalink":"http://www.sillystone.info/tags/jetty/"},{"name":"WAS","slug":"WAS","permalink":"http://www.sillystone.info/tags/WAS/"},{"name":"java","slug":"java","permalink":"http://www.sillystone.info/tags/java/"}]},{"title":"db2 实施总结","date":"2017-03-19T01:53:26.000Z","path":"2017/03/19/db2-实施总结/","text":"针对参与实施的两个项目的数据库部分，总结如下本文示例基于db2 10.5.0 物理设计表分区（Partition）表分区主要为了数据隔离，按分区操作效率高，按分区运维方便 建表分区按分区字段键值区间设置1234567891011121314CREATE TABLE \"SCMNAME\".\"T_TABNAME\" ( \"PART_COL\" VARCHAR(12 OCTETS) , \"TAB_COL\" VARCHAR(32 OCTETS) , \"TAB_COL2\" DECIMAL(24,6) , \"REMARKS\" VARCHAR(240 OCTETS)) IN \"TBS_DATA_8K\" PARTITION BY RANGE(PART_COL) ( STARTING '001' ENDING '002', STARTING '003' ENDING '200', STARTING '201' ENDING '400', ENDING MAXVALUE ) 每个分区可以指定表空间和索引空间，上述sql db2look导出后1234567 PARTITION BY RANGE(PART_COL) ( STARTING '001' ENDING '002' IN \"TBS_DATA_8K\", STARTING '003' ENDING '200' IN \"TBS_DATA_8K\", STARTING '201' ENDING '400' IN \"TBS_DATA_8K\", ENDING MAXVALUE IN \"TBS_DATA_8K\") 索引默认为分区索引1234```主键/唯一索引 是全局索引``` sql 运维清除某分区（比如：按日期分区）的数据12 日志日志模式db2 默认为循环日志，开发和测试环境一般设置为循环日志，不可前滚恢复 不需要运维归档日志，生产环境一般为此模式，模式选项包括：RETAIN， DISK 需指定日志路径 123456db2 update db cfg for USER_DB_NAME using LOGARCHMETH1 DISK:/db2arclog-- 修改后需要备份和重启数据库db2stop forcedb2startdb2 backup db USER_DB_NAME to /disk 日志路径缺省日志路径：/home/dbuser/dbinst/NODE0000/SQL00001/LOGSTREAM0000修改：1db2 update db cfg for USER_DB_NAME using NEWLOGPATH /db2actlog 日志大小db2 操作时，可以认为一个事物内的所有数据库记录都在日志中记录， 如果某操作（特别是批量操作）处理了大量数据，那么将使用大量的数据库日志存储。 日志总大小=单个日志大小 (主日志个数 + 辅日志个数)LOGFILSZ (LOGPRIMARY + LOGSECOND) 数据库启动时 LOGPRIMARY的空间已分配，如果磁盘空间不足，将导致数据库无法启动123db2 update db cfg for USER_DB_NAME using LOGFILSZ 4096;db2 update db cfg for USER_DB_NAME using LOGPRIMARY 20;db2 update db cfg for USER_DB_NAME using LOGSECOND 30; 缓冲区应用设计问题解决db2diag 查看数据库运行日志(1) A timestamp for the message.(2) The name of the instance generating the message.(3) For DB2 Extended Enterprise Edition systems with a db2nodes.cfg file, the node generating the message. (If the db2nodes.cfg file is not used, the value is “000”.)(4) Identification of the process was generating the message. In this example, the message came from the process identified as 44829. The name of this process is db2agent and it is connected to the database named SAMPLE. Note: If the application is operating in a DUOW environment, the ID shown is the DUOW correlation token.(5) Identification of the application for which the process is working. In this example, the process generating the message is working on behalf of an application with the ID *LOCAL.payroll.970317140834. To identify more about a particular application ID, either: Use the db2 list applications command to view a list of application IDs. From this list, you can determine information about the client experiencing the error, such as its node name and its TCP/IP address. Use the db2 get snapshot for application command to view a list of application IDs.(6) The DB2 component that is writing the message.(7) The name of the function that is providing the message. This function operates within the DB2 subcomponent that is writing the message. To find out more about the type of activity performed by a function, look at the fourth letter of its name. In this example, the letter “p” in the function “sqlplrq” indicates a data protection problem. (Logs could be damaged, for example.) The following list shows some of the letters used in the fourth position of the function name, and the type of activity they identify: b Buffer poolsc Communication between clients and serversd Data managemente Engine processeso Operating system calls (such as opening and closing files)p Data protection (such as locking and logging)r Relational database servicess Sortingx Indexing (8) Identification of the internal error that was reported.(9) The database on which the error occurred.(10) Diagnostic message indicating that an internal error occurred.(11) Hexadecimal representation of an internal return code (see “Interpreting Hexadecimal Codes”). db2 “?” sql0912运维管理备份/恢复123db2 connect to SAMPLEdb2 -x \"select 'REBIND PACKAGE ' || rtrim(pkgschema) || '.' || rtrim(pkgname) || ';' as command from syscat.packages\" &gt; rebind.sqldb2 -tvf rebind.sql","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"db2","slug":"db2","permalink":"http://www.sillystone.info/tags/db2/"},{"name":"数据库","slug":"数据库","permalink":"http://www.sillystone.info/tags/数据库/"}]},{"title":"mybatis 汇总","date":"2017-03-17T02:45:21.000Z","path":"2017/03/17/mybatis-汇总/","text":"CDATA的使用在XML文档中的所有文本都会被解析器解析，只有在CDATA部件之内的文本会被解析器忽略。术语 CDATA 指的是不应由 XML 解析器进行解析的文本数据（Unparsed Character Data）。 在 XML 元素中，”&lt;” 和 “&amp;” 是非法的。“&lt;” 会产生错误，因为解析器会把该字符解释为新元素的开始。“&amp;” 也会产生错误，因为解析器会把该字符解释为字符实体的开始 某些文本，比如 JavaScript 代码，包含大量 “&lt;” 或 “&amp;” 字符。为了避免错误，可以将脚本代码定义为 CDATA。 CDATA 部分中的所有内容都会被解析器忽略。CDATA 部分由 “&lt;![CDATA[“ 开始，由 “]]&gt;” 结束CDATA 部分不能包含字符串 “]]&gt;”。也不允许嵌套的 CDATA 部分。标记 CDATA 部分结尾的 “]]&gt;” 不能包含空格或折行。&lt;![CDATA[ ]]&gt; 标记避免Sql中与xml规范相冲突的字符对xml映射文件的合法性造成影响","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://www.sillystone.info/tags/mybatis/"}]},{"title":"db2 权限管理","date":"2017-03-15T12:39:19.000Z","path":"2017/03/15/db2-权限管理/","text":"背景1.用户分类设置权限：查询用户，数据库管理用户，数据库maintain用户2.数据库使用restrictive模式创建 参考资料restrictive模式对数据库权限的影响参照IBM Securing the system catalog viewPUBLIC无任何权限，非restrictive模式会默认包含以下权限 CREATETAB BINDADD CONNECT IMPLICIT_SCHEMA EXECUTE with GRANT on all procedures in schema SQLJ EXECUTE with GRANT on all functions and procedures in schema SYSPROC BIND on all packages created in the NULLID schema EXECUTE on all packages created in the NULLID schema CREATEIN on schema SQLJ CREATEIN on schema NULLID USE on table space USERSPACE1 SELECT access to the SYSIBM catalog tables SELECT access to the SYSCAT catalog views SELECT access to the SYSIBMADM administrative views SELECT access to the SYSSTAT catalog views UPDATE access to the SYSSTAT catalog views dbadm用户数据库对象（表，试图，索引等常规对象）维护权限 12345678db2 connect to $DBNAMEdb2 drop role $ADMROLEdb2 create role $ADMROLEdb2 grant dbadm on database to role $ADMROLEdb2 grant role $ADMROLE to $TARGET_USER## another userdb2 grant role $ADMROLE to $ANOTHER_USER query用户 最小权限的查询用户， 无法看到系统视图（schema，数据库对象列表信息） 123456789101112131415161718db2 connect to $DBNAMEdb2 drop role $QRYROLEdb2 create role $QRYROLEdb2 grant connect on database to role $QRYROLEdb2 grant usage on workload sysdefaultuserworkload to role $QRYROLEdb2 grant execute on package NULLID.SQLC2K26 to role $QRYROLEdb2 grant execute on package NULLID.SYSSH200 to role $QRYROLEdb2 grant select on table SYSCAT.SCHEMATA to role $QRYROLEdb2 grant select on table SYSIBM.SYSDUMMY1 to role $QRYROLE# your tablefor TAB_NAME in `db2 -x \"select trim(TABSCHEMA)||'.'||TABNAME from SYSCAT.TABLES where TABSCHEMA = 'YOUR_SCHEMA'\"`do echo \"$TAB_NAME\" db2 \"grant select on table $TAB_NAME to role $QRYROLE\"donedb2 grant role $QRYROLE to user $TARGET_USER 查询用户，可以查看数据库系统视图（使用DbVisualizer工具）1234567891011121314db2 connect to $DBNAMEdb2 drop role $QRYROLEdb2 create role $QRYROLEdb2 grant dbadm without dataaccess on database to role $QRYROLEdb2 grant execute on package NULLID.SYSSH200 to role $QRYROLE# your tablefor TAB_NAME in `db2 -x \"select trim(TABSCHEMA)||'.'||TABNAME from SYSCAT.TABLES where TABSCHEMA = 'YOUR_SCHEMA'\"`do echo \"$TAB_NAME\" db2 \"grant select on table $TAB_NAME to role $QRYROLE\"donedb2 grant role $QRYROLE to user $TARGET_USER 问题和解决方法 查询用户的权限赋值过程 对于NULLID package的权限根据错误提示执行 DbVisualizer工具报错问题 : 设置Tools-&gt;DebugWindow log Destination查看错误输出 错误提示： SQLCODE=-551, SQLSTATE=42501, SQLERRMC=DB2USER;EXECUTE;SYSIBM.SQLTABLES （未能通过赋权解决） DB2工具： db2top watch功能 导出dynamic sqlDB2 Diaglog: db2 update dbm cfg using DIAGLEVEL 4; check $DIAGPATH/db2diag.log 原因： call SYSIBM.SQLTABLES() 无 execute权限grant execute on procedure SYSIBM.SQLTABLES to user TarGETUSER系统catelog package/procedure无法赋值给普通用户， 报错sqlcode: -607 sqlstate: 42832参考IBM 错误说明 网上的权限分配示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394#--创建角色 bsb_write_role：具有DML操作权限 bsb_read_role：只读权限db2 create role bsb_write_roledb2 create role bsb_read_role#-----------------------------------------------##--给角色 bsb_write_role 授权db2 grant usage on workload sysdefaultuserworkload to role bsb_write_roledb2 grant connect on database to role bsb_write_roledb2 grant bindadd on database to role bsb_write_roledb2 grant load on database to role bsb_write_roledb2 grant create_external_routine on database to role bsb_write_roledb2 grant createtab on database to role bsb_write_roledb2 grant use of tablespace userspace1 to role bsb_write_roledb2 grant implicit_schema on database to role bsb_write_role#db2 grant dataaccess on database to role bsb_write_role#问题：如何确定应用用户需要哪些package的执行权限？db2 grant execute on package nullid.sqlc2j25 to role bsb_write_roledb2 grant execute on package nullid.syssh200 to role bsb_write_roledb2 grant execute on package nullid.sqlubj05 to role bsb_write_roledb2 grant execute on package nullid.sqlukj0b to role bsb_write_roledb2 grant execute on package nullid.sqlupj00 to role bsb_write_roledb2 grant execute on package nullid.sqlucj05 to role bsb_write_roledb2 grant execute on package nullid.sqluaj20 to role bsb_write_roledb2 grant execute on package nullid.sqlufj14 to role bsb_write_roledb2 grant execute on package nullid.sqluoj01 to role bsb_write_roledb2 grant execute on function sysproc.base_table to role bsb_write_roledb2 grant select on table syscat.colidentattributes to role bsb_write_roledb2 grant select on table sysibmadm.dbcfg to role bsb_write_roledb2 grant select on table sysibm.systables to role bsb_write_roledb2 grant select on table sysibm.sysindexes to role bsb_write_roledb2 grant select on table sysibm.syscolumns to role bsb_write_roledb2 grant select on table sysibm.dual to role bsb_write_roledb2 grant select on table syscat.packages to role bsb_write_roledb2 grant select on table syscat.columns to role bsb_write_roledb2 grant select on table syscat.indexcoluse to role bsb_write_roledb2 grant select on table syscat.sequences to role bsb_write_roledb2 grant select on table syscat.functions to role bsb_write_roledb2 grant select on table syscat.tables to role bsb_write_roledb2 grant select on table syscat.tabauth to role bsb_write_roledb2 grant select on table syscat.tbspaceauth to role bsb_write_roledb2 grant select on table syscat.views to role bsb_write_roledb2 grant select on table syscat.schemaauth to role bsb_write_roledb2 grant select on table syscat.sequences to role bsb_write_roledb2 grant select on table syscat.sequenceauth to role bsb_write_roledb2 grant select on table syscat.roles to role bsb_write_roledb2 grant select on table syscat.roleauth to role bsb_write_roledb2 grant select on table syscat.procedures to role bsb_write_roledb2 grant select on table syscat.references to role bsb_write_roledb2 grant select on table syscat.packages to role bsb_write_roledb2 grant select on table syscat.packageauth to role bsb_write_role#-----------------------------------------------##--给角色 bsb_read_role 授权db2 grant connect on database to role bsb_read_roledb2 grant select on table syscat.tables to role bsb_read_roledb2 grant select on table syscat.tabauth to role bsb_read_roledb2 grant select on table syscat.tbspaceauth to role bsb_read_roledb2 grant select on table syscat.views to role bsb_read_roledb2 grant select on table syscat.schemaauth to role bsb_read_roledb2 grant select on table syscat.sequences to role bsb_read_roledb2 grant select on table syscat.sequenceauth to role bsb_read_roledb2 grant select on table syscat.roles to role bsb_read_roledb2 grant select on table syscat.roleauth to role bsb_read_roledb2 grant select on table syscat.procedures to role bsb_read_roledb2 grant select on table syscat.references to role bsb_read_roledb2 grant select on table syscat.packages to role bsb_read_roledb2 grant select on table syscat.packageauth to role bsb_read_roledb2 grant select on table sysibm.dual to role bsb_read_role#-----------------------------------------------##--创建模式# 1）没有隐式模式权限（IMPLICIT_SCHEMA）的用户必须显示创建模式 # 2）没有DBADM权限的应用用户bsbview可以创建与用户名同名的模式bsbviewbsbview@sles11:~&gt; db2 \"create schema bsbview\"DB20000I The SQL command completed successfully.#--授权模式bsbview的权限给角色bsb_write_roledb2 grant createin,alterin,dropin on schema bsbview to role bsb_write_role#--理解以下两个概念很重要！# 1）对象的创建者自动拥有了该对象的所有权限。# 2）用户拥有模式的DML权限后，在该模式上就拥有了创建对象的权限。# 3）对象包含：表，视图，索引，序列，触发器，存储过程，函数#--给应用用户授权角色bsb_write_roledb2 grant role bsb_write_role to user bsbview#-----------------------------------------------##回收创建表的权限后，使用表空间的权限也将默认回收：db2 revoke CREATETAB on DATABASE from bsbview #———————————————-# 设置应用要连接的实例的环境变量#———————————————-#1）DB2实例查看方法cd /opt/IBM/db2/V10.1/instance./db2ilistdb2inst1 #DB2的实例名其实是操作系统的一个用户名2）查看实例 db2inst1 家目录cat /etc/passwd|grep db2inst1db2inst1:x:1001:1000::/home/db2inst1:/bin/bash3）修改应用用户的 .profilebsbview@sles11:~&gt; cat &gt;&gt; ~/.profile &lt; if [ -f /home/db2inst2/sqllib/db2profile ]; then . /home/db2inst2/sqllib/db2profilefiEOF #———————————————-# END 设置应用要连接的实例的环境变量#———————————————-# #———————————————————————————-# 最小化权限管理实验#———————————————————————————-# #–建库语句，必须用 RESTRICTIVE 参数db2 “create database test2 on /db2data1,/db2data2,/db2data3 using codeset UTF-8 territory cn RESTRICTIVE” #–DB2数据库rest为restrict模式db2inst2@sles11:~&gt; db2 get db cfg |grep -i restrictRestrict access = YES 对于没有任何权限的OS用户bsbview，执行如下操作报错的解决方法： 1)没有connect权限bsbview@sles11:~&gt; db2 connect to restSQL1060N User “BSBVIEW “ does not have the CONNECT privilege. SQLSTATE=08004 解决方法：db2 grant connect on database to bsbview bsbview@sles11:~&gt; db2 connect to rest Database Connection Information Database server = DB2/LINUXX8664 10.1.3SQL authorization ID = BSBVIEWLocal database alias = REST 2)列出模式bsbview的表，没有workload权限bsbview@sles11:~&gt; db2 list tablesSQL5193N The current session user does not have usage privilege on anyenabled workloads. SQLSTATE=42524 解决方法：db2 grant usage on workload sysdefaultuserworkload to user bsbview 3)没有执行包NULLID.SQLC2J25权限bsbview@sles11:~&gt; db2 list tablesSQL0551N “BSBVIEW” does not have the required authorization or privilege toperform operation “EXECUTE” on object “NULLID.SQLC2J25”. SQLSTATE=42501 解决方法：db2 GRANT EXECUTE ON PACKAGE NULLID.SQLC2J25 TO bsbview 4)没有视图syscat.tables的查询权限bsbview@sles11:~&gt; db2 list tablesSQL0551N “BSBVIEW” does not have the required authorization or privilege toperform operation “SELECT” on object “SYSCAT.TABLES”. SQLSTATE=42501 解决方法：db2 grant select on table syscat.tables to user bsbview 通过以上4种权限：用户bsbview可以正常连接上rest，并可以列出模式bsbview下表：bsbview@sles11:~&gt; db2 list tables for schema bsbview Table/View Schema Type Creation time 0 record(s) selected. 5）没有create table权限bsbview@sles11:~&gt; db2 “create table t1(id int)”DB21034E The command was processed as an SQL statement because it was not avalid Command Line Processor command. During SQL processing it returned:SQL0552N “BSBVIEW” does not have the privilege to perform operation “CREATETABLE”. SQLSTATE=42502 解决方法：db2 grant CREATETAB ON DATABASE to bsbview 6）没有隐式的创建模式权限：IMPLICIT CREATE SCHEMAbsbview@sles11:~&gt; db2 “create table t1(id int)”DB21034E The command was processed as an SQL statement because it was not avalid Command Line Processor command. During SQL processing it returned:SQL0552N “BSBVIEW” does not have the privilege to perform operation “IMPLICITCREATE SCHEMA”. SQLSTATE=42502 解决方法：对于没有IMPLICIT_SCHEMA权限的用户，有两种解决办法：1）直接授予IMPLICIT_SCHEMA权限：db2 grant IMPLICIT_SCHEMA ON DATABASE to user bsbview2）使用DBADM的用户创建bsbview所需要的模式，然后授权db2 create schema s1db2 grant createin,alterin,dropin on schema s1 to user bsbview 7）没有表空间权限，若不指定表空间名字，默认使用表空间USERSPACE1bsbview@sles11:~&gt; db2 “create table s1.t1(id int) in userspace1”DB21034E The command was processed as an SQL statement because it was not avalid Command Line Processor command. During SQL processing it returned:SQL0551N “BSBVIEW” does not have the required authorization or privilege toperform operation “CREATE TABLE” on object “USERSPACE1”. SQLSTATE=42501 解决方法： db2 grant use of TABLESPACE USERSPACE1 to bsbview bsbview@sles11:~&gt; db2 “create table s1.t1(id int) in userspace1”DB20000I The SQL command completed successfully. 8）没有存储过程执行的权限bsbview@sles11:~&gt; db2 “call s1.sleep(10)”SQL0551N “BSBVIEW” does not have the required authorization or privilege toperform operation “EXECUTE” on object “NULLID.SYSSH200”. SQLSTATE=42501 解决方法： db2 grant execute on package NULLID.SYSSH200 to user bsbview bsbview@sles11:~&gt; db2 “call s1.sleep(2)” Return Status = 0 9）没有export权限bsbview@sles11:~&gt; db2 “export to s1.t1.ixf of ixf messages s1.t1.msg select * from s1.t1”SQL3020N The user does not have the authority to run the specified EXPORTcommand.bsbview@sles11:~&gt; lltotal 8drwxr-xr-x 2 bsbview users 4096 Feb 1 17:04 bin-rw-r–r– 1 bsbview users 719 Feb 4 12:24 s1.t1.msgbsbview@sles11:~&gt; cat s1.t1.msgSQL3015N An SQL error “-551” occurred during processing. SQL0551N “BSBVIEW” does not have the required authorization or privilege toperform operation “EXECUTE” on object “NULLID.SQLUBJ05”. SQLSTATE=42501 SQL3015N An SQL error “-551” occurred during processing. SQL0551N “BSBVIEW” does not have the required authorization or privilege toperform operation “EXECUTE” on object “NULLID.SQLUKJ0B”. SQLSTATE=42501 SQL3015N An SQL error “-551” occurred during processing. SQL0551N “BSBVIEW” does not have the required authorization or privilege toperform operation “EXECUTE” on object “NULLID.SQLUPJ00”. SQLSTATE=42501 SQL3020N The user does not have the authority to run the specified EXPORTcommand. 解决方法：db2 grant execute on package nullid.sqlubj05 to user BSBVIEWdb2 grant execute on package nullid.sqlukj0b to user BSBVIEWdb2 grant execute on package nullid.sqlupj00 to user BSBVIEWdb2 grant execute on package nullid.sqlucj05 to user BSBVIEWdb2 grant execute on package nullid.sqluaj20 to user BSBVIEWdb2 grant execute on function sysproc.base_table to user BSBVIEWdb2 grant select on table SYSCAT.COLIDENTATTRIBUTES to user BSBVIEWdb2 grant select on table SYSCAT.INDEXCOLUSE to user BSBVIEWdb2 grant select on table SYSCAT.SEQUENCES to user BSBVIEWdb2 grant select on table SYSIBM.SYSTABLES to user BSBVIEWdb2 grant select on table SYSIBM.SYSINDEXES to user BSBVIEWdb2 grant select on table syscat.functions to user BSBVIEWdb2 grant select on table sysibm.syscolumns to user BSBVIEW –授予上面的权限后，最终报错：bsbview@sles11:~&gt; cat s1.t1.msgSQL3104N The Export utility is beginning to export data to file “s1.t1.ixf”. SQL27981W The utility could not verify presence of attached or detached datapartitions in the target table or the source table. SQL0551N “” does not have the required authorization or privilege to performoperation “” on object “”. SQL3105N The Export utility has finished exporting “1” rows. 10）没有import权限bsbview@sles11:~&gt; db2 “import from s1.t1.ixf of ixf insert into s1.t1”SQL0551N “BSBVIEW” does not have the required authorization or privilege toperform operation “EXECUTE” on object “NULLID.SQLUFJ14”. SQLSTATE=42501 bsbview@sles11:~&gt; db2 “import from s1.t1.ixf of ixf insert into s1.t1”SQL0551N “BSBVIEW” does not have the required authorization or privilege toperform operation “SELECT” on object “SYSIBMADM.DBCFG”. SQLSTATE=42501 bsbview@sles11:~&gt; db2 “import from s1.t1.ixf of ixf insert into s1.t1”SQL3015N An SQL error “-551” occurred during processing. SQL0551N “BSBVIEW” does not have the required authorization or privilege toperform operation “EXECUTE” on object “NULLID.SQLUOJ01”. SQLSTATE=42501 bsbview@sles11:~&gt; db2 “import from s1.t1.ixf of ixf insert into s1.t1”SQL3015N An SQL error “-551” occurred during processing. SQL0551N “BSBVIEW” does not have the required authorization or privilege toperform operation “SELECT” on object “SYSCAT.PACKAGES”. SQLSTATE=42501 SQL3015N An SQL error “” occurred during processing. 解决方法：db2 grant execute on package nullid.SQLUFJ14 to user BSBVIEWdb2 grant select on table SYSIBMADM.DBCFG to user BSBVIEWdb2 grant execute on package NULLID.SQLUOJ01 to user BSBVIEWdb2 grant select on table SYSCAT.PACKAGES to user BSBVIEWdb2 grant select on table SYSCAT.COLUMNS to user BSBVIEW bsbview@sles11:~&gt; db2 “import from s1.t1.ixf of ixf insert into s1.t1”SQL27981W The utility could not verify presence of attached or detached datapartitions in the target table or the source table. SQL3150N The H record in the PC/IXF file has product “DB2 02.00”, date“20150204”, and time “124100”. SQL3153N The T record in the PC/IXF file has name “s1.t1.ixf”, qualifier “”,and source “ “. SQL3015N An SQL error “-551” occurred during processing. SQL0551N “BSBVIEW” does not have the required authorization or privilege toperform operation “SELECT” on object “SYSCAT.COLUMNS”. SQLSTATE=42501 SQL3110N The utility has completed processing. “0” rows were read from theinput file. #———————————————————————————-# END 最小化权限管理实验#———————————————————————————-#","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"db2","slug":"db2","permalink":"http://www.sillystone.info/tags/db2/"}]},{"title":"db2数据库物理设计要点","date":"2017-03-14T08:59:52.000Z","path":"2017/03/14/db2数据库设计要点/","text":"整体架构单分区SMP集群数据库Purescale多分区MPP：数据量大于2TB 数据库设计建库 库名：XXXXDB数据文件存储目录： 独立挂载，便于运维和扩展数据库主目录：独立挂载，空间需求：小于10g活动日志目录：独立挂载，容量设计（压测大小的1.5倍）归档日志目录：独立挂载，容量设计（至少3天日志量，循环日志不需要配置）代码页： UTF-8（1208）RESTRICTIVE：如果未声明 则默认 restrictive 为no，默认赋值PUBLIC权限（下列数据库特权被自动授予 PUBLIC：对系统目录视图的 CREATETAB、BINDADD、CONNECT、IMPLICIT_SCHEMA 和 SELECT）。但是，如果有 RESTRICTIVE 选项，那么不会自动对 PUBLIC 授予任何特权.在此情况下对于应用服务器访问用户建议授权admin权限，对于其他用户单独授权，授权示例见下文PAGESIZE：单位为k，此选项设置默认的buffer pool，影响table spaces (SYSCATSPACE, TEMPSPACE1, USERSPACE1)，同时也是默认的buffer pool，table spaces属性 12db2 create database &lt;dbname&gt; on ‘/db2fs1’,’/db2fs2’dbpath on‘/dbpath’using codeset UTF-8 territory CN pagesize 8192 restrictive 12345db2 \"grant dbadm on database to user APPUSERNAME\"db2 \"grant connect on database to user $TARGET_USER\"db2 \"grant usage on workload sysdefaultuserworkload to user $TARGET_USER\"db2 \"grant select,insert,update,delete on table $TAB_NAME to user $TARGET_USER\" 缓冲池 大小不应该超过数据库可用内存50%与表空间设计同时考虑缓冲池个数尽量少缺省缓冲 调整为固定大小 12create bufferpool &lt;bpname&gt; size &lt;n&gt; pagesize 8k;alter bufferpool ibmdefaultbp size 25600; 表空间设计日志设计索引设计设计专题容量设计示例语句12","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"db2","slug":"db2","permalink":"http://www.sillystone.info/tags/db2/"},{"name":"数据库","slug":"数据库","permalink":"http://www.sillystone.info/tags/数据库/"}]},{"title":"db2分区特性","date":"2017-03-13T08:09:56.000Z","path":"2017/03/13/db2分区特性/","text":"概述数据分区技术主要用于大容量数据的物理设计，主要特性包括：访问速度，数据隔离，数据扩展和数据运维几个方面。 架构上可以分为：共享内存， 共享存储， 无共享。 DPF 数据分区Data Partitioning feature， 采用share-nothing体系结构。数据库在非共享环境下分为独立的分区，每个分区有独立的硬件资源，数据，索引和日志，每个数据分区可以称为节点；每个节点有独立处理能力，节点间通过高速网路连接。 扩展性好，可增加节点。一库拆分为多库的思路。并行策略，节点之间，节点内分区并行，查询语句并行DPF还根据负载动态分流，同时支持异步I/O和平行I/O MDCIBM MDC 介绍MDC是在DB2 Version 8中引入的，通过它可以在物理上将在多维上具有类似值的行聚合在一起放在磁盘上。这种聚合能为常见分析性查询提供高效的I/O，提高检索数据的效率一个块的大小等于表空间的扩展数据块（extent）大小，扩展数据块是磁盘上的一组连续页，所以将这些具有类似值的行在物理上是存放在连续的数据页上在 MDC 表中，块映射（block map）会跟踪属于这个表的所有扩展数据块，并且指示哪些块或扩展数据块上包含数据以及哪些块或扩展数据块上没有包含数据。包含数据的块标记为“正在使用”(“IN USE”)。每当发生删除或转出时，相应的块条目不再标记为“正在使用”，而是被释放以供 MDC 表复用。但是表空间中的其他对象无法使用这些扩展数据块。可以通过重组 MDC 表来从 MDC 表释放这些可用数据扩展数据块 特点 由于与基于记录的索引相比，由于块索引的大小是很小的，所以探测和扫描块索引要快 块索引和相应的数据组织允许更精细的“数据库分区忽略”或者有选择性地进行表访问 利用块索引的查询因为减小了索引大小、优化了对块的预取并且可以保证相应数据的集群而受益 对于某些查询，可以减少锁定和谓词求值 块索引用于日志记录和维护方面的开销很少，因为仅当将第一条记录添加至块或从块中除去最后一条记录时才需要更新它们 转入的数据可以复用先前转出的数据留下的连续空间。 适用场景数据量大，经常按某一条件删除或者增加数据，经常使用相异值小的列（这样的列不适合常规索引）进行数据范围筛选 维度筛选 用于范围、等于或 IN 列表谓词用于转入、转出或其他大规模的行删除被 GROUP BY 或 ORDER by 子句引用外键列星型数据库的事实表中 join 子句中的列粗粒度，也就是说不同的值很少的列典型的设计是用一个表示日期的列作为一个 MDC 维，再加上 0 到 3 个其他列作为其他维，例如 region 和 product_type。绝对不能使用主键或唯一索引列做维度 索引设计MDC表支持创建常规索引，对于一个n维的MDC表自动会创建n+1个索引，即：每个维度一个索引，加上所有维度的组合索引 TP表分区TP是在DB2 9中引入的，与MDC类似，它也可以将具有近似值的行存储在一起。TP 不同于其他特性的优势在于为表添加或删除大量数据这个方面，即转入和转出。分区表的索引全部建成分区索引，要求尽量不要建立主键和唯一索引 分区表的索引默认是分区索引, 如下所示：12345678CREATE TABLE A (columns) PARTITION BY RANGE (column expression) (PARTITION PART0 STARTING FROM constant ENDING constant IN ts1 INDEX IN ts2, PARTITION PART1 STARTING FROM constant ENDING constant IN ts3 INDEX IN ts4, PARTITION PART2 STARTING FROM constant ENDING constant IN ts3,INDEX IN ts5) CREATE INDEX x1 ON A (...); CREATE INDEX x2 ON A (...); 分区表建立非分区索引，下例x1; 存储空间参见示例图21234567891011CREATE TABLE t1 (columns) in ts1 INDEX IN ts2 1 PARTITION BY RANGE (column expression) (PARTITION PART0 STARTING FROM constant ENDING constant IN ts3, 2 PARTITION PART1 STARTING FROM constant ENDING constant INDEX IN ts5, PARTITION PART2 STARTING FROM constant ENDING constant INDEX IN ts4, PARTITION PART3 STARTING FROM constant ENDING constant INDEX IN ts4, PARTITION PART4 STARTING FROM constant ENDING constant)CREATE INDEX x1 ON t1 (...) NOT PARTITIONED; CREATE INDEX x2 ON t1 (...) PARTITIONED;CREATE INDEX x3 ON t1 (...) PARTITIONED; 空间存储示例如下： 比较DDL 分区方式 Create table语句 DPF DISTRIBUTE BY HASH MDC ORGANIZE BY DIMENSION TP PARTITION BY RANGE 设计特性 分区方式 实现思路 优点 DPF 将行均匀地分布在多个数据库分区上 可伸缩性随着数据库的增长增加计算资源（也就是数据库分区） MDC 将在多维上具有近似值的行放在表中相同的物理位置，即所谓的块 查询性能 —— 组织数据的方式有利于获得更快的检索速度，对于由多个谓词指定范围的查询尤其有效 TP 将所有行放在同一个数据分区的一个指定范围的维中 数据移动 —— 通过添加和删除整个数据分区，可以增加和删除大量数据 事实表特征 分区方式 适合 事实 DPF 大型表 —— 大到无法仅依靠单独一组 CPU 和 I/O 通道来处理 事实表是最大的数据库表。它们常常包含数亿行数据，有时候甚至包含数千亿行数据 MDC 结果集返回在多个维上具有近似值的行的查询 事实表（以及通常所说的数据仓库）是为支持这种类型的查询而设计的 TP 这种类型的表：周期性地添加大量数据，然后在数据到期后又删除大量数据 在事实表中，常常是每天都添加新数据。通常每月或每个季度删除过时的数据 经验法则总结 分区方式 分区特性设计决定 经验法则 DPF 用作分布键的列 首选是具有很多不同值的列 MDC 用作 MDC 维的列 一种典型的设计是选择一个表示日期的列，再加上 0 到 3 个其他列 TP 用作表分区键的列和分区的数量 选择一个基于时间的列。定义与每次转出的数据量相符的分区 DB2 &amp; Oracle Oracle分区 DB2分区 Oracle 10g语法 DB2 V9语法 区间分区（Range Partitioning） 表分区（Table Partitioning） PARTITION BY RANGE PARTITION BY RANGE 哈希分区（Hash Partitioning） 数据库分区（Database Partitioning） PARTITION BY HASH DISTRIBUTE BY HASH 列表分区（List Partitioning） 带生成列表分区（Table Partitioning With Generated Column） PARTITION BY LIST PARTITION BY RANGE 不支持 多维集群（Multidimensional clustering） 无 ORGANIZE BY DIMENSION DB2的数据库分区特性采用Share-nothing架构，这种架构允许多个数据库分区在一起并行工作来处理工作负载。在Oracle中，使用Share-disk架构 代码示例1234567891011 CREATE TABLE partition_table (partition_date date NOT NULL, partition_data VARCHAR(20) NOT NULL ) IN tbsp_parts DISTRIBUTE BY HASH (partition_date); 参考IBM DB2关键特性解析：DB2分区特性IBM官网： 表分区和多维集群表","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"db2","slug":"db2","permalink":"http://www.sillystone.info/tags/db2/"},{"name":"数据库","slug":"数据库","permalink":"http://www.sillystone.info/tags/数据库/"}]},{"title":"思维","date":"2017-03-12T14:13:34.000Z","path":"2017/03/12/思维/","text":"回归理性Learn the rules like a pro, so you can break them like an artist 已知观点 专注 时间观 写下你的想法 分享也是学习， 乐于奉献 把握重点 不打”价格战“ 内部”压力测试“ 投资自己， 包括”眼界“ 先调整”状态“ 再做”事“ 最不在乎输赢的，往往是赢家 创造属于自己的细分领域 有待理解不要高估失败 （重视结果）从失败中，你可能学不到任何东西；失败除了是悲剧，什么也不是Pivot 意味着已经走错了 反对谈趋势趋势意味着竞争， 不如 使命感 少一些竞争力我在学校里成绩非常好，个人竞争力极强。但回头看，我不希望自己眼中只有竞争，也不想把精力全部放在竞争对手身上；那样我会错过其他很多东西，忽略了创造更有价值的事情。我希望自己少一些竞争力，从而变得更加成功 向对立面学习除了学习硅谷的同行，我会花很多时间学习价值投资者，比如沃伦·巴菲特、塞思·卡拉曼等。我们与价值投资是两种截然相反的投资类型：我们是押注“变化”，价值投资是押注“不变”。如果他们犯错，则意味着某些变化他们没有意识到；如果我们犯错，则意味着我们预测的变化还没有出现。但两者的共同之处在于：我们都倡导原创性思考，去挖掘别人看不到、甚至持相反观点的机会 不对称的风险与回报“风险越大，回报越高”几乎是所有投资者的共识。但最伟大的投资人都在挖空心思以最小的风险（甚至无风险），获得最高的回报。","categories":[{"name":"鸡汤","slug":"鸡汤","permalink":"http://www.sillystone.info/categories/鸡汤/"}],"tags":[{"name":"理念","slug":"理念","permalink":"http://www.sillystone.info/tags/理念/"}]},{"title":"严歌苓的文字","date":"2017-03-11T15:26:31.000Z","path":"2017/03/11/严歌苓的文字/","text":"述简洁的文字，不玩弄词藻，表达清晰而直接，具有冲击性心理/思想描述很到位，也是她作品最吸引人的地方故事叙述的切入也是其文字特点，快速切入，却不影响连贯性很佩服她可以把心理和性格表达的如此流畅作品把人物的丰富情感和现实的生活间的冲突和结合展示的很到位 作品已读：《小姨多鹤》 人物性格到位，多鹤，小环，张检，张钢的刻画栩栩如生，折射出时代背景和民族性格 结尾感觉有些局促和拖拉，对于张铁的刻画不够清晰，结局似乎无亮点无冲击。《寄居者》 喜欢她的文字始于这本书，民族性格的展现以及人物道德底线的冲击是本书的精华《霜降》 和上边作品相比算是短篇，小环境，小人物，故事完整性很好。《天浴》 尾好的作品不多，好的作者更少，珍惜佳作","categories":[{"name":"随笔","slug":"随笔","permalink":"http://www.sillystone.info/categories/随笔/"}],"tags":[{"name":"日记","slug":"日记","permalink":"http://www.sillystone.info/tags/日记/"}]},{"title":"浅谈银行系统-渠道类系统","date":"2017-03-11T01:18:00.000Z","path":"2017/03/11/浅谈银行系统-渠道类系统/","text":"概述银行的渠道类系统按照先后发展有以下几类： 传统渠道：柜面 电话 ATM 网银 银企直连新兴渠道：手机 微信（企业号，小程序） 自助银行 智能终端设备 对客接入点先后经历了：实体网点，自助设备，电话银行，网银，手机等智能设备伴随着智能设备的普及和安全技术以及大众的认知改变，客户接入渠道逐渐转向智能设备对客服务方式也逐渐转变。 银行的渠道不仅是服务客户的接入点，同时也是银行营销的途径，随着零售业务的发展，各银行都很重视各种渠道的客户获取和产品营销，获客渠道也逐渐成为各家银行的一个重要渠道系统；这也成就了目前互联网公司和银行的合作，同时银行的产品也通过第三方互联网公司开展营销，这其中包含：渠道与信誉的整合，利益分割，资源积累，公司发展策略 直销银行：主要通过互联网渠道提供高效，便捷，有特色的服务。主要营收可以依赖于：贷款， 金融产品收益。贷款产品主要是信用贷，和阿里等公司的产品同质，如无特色很难超越金融产品创新，主要包括金融超市直营方式销售金融产品，需要有特色的金融产品扩展用户 渠道类系统(待完善)柜面网银手机银行微信银行渠道整合渠道技术主题安全","categories":[{"name":"金融","slug":"金融","permalink":"http://www.sillystone.info/categories/金融/"}],"tags":[{"name":"金融","slug":"金融","permalink":"http://www.sillystone.info/tags/金融/"},{"name":"银行","slug":"银行","permalink":"http://www.sillystone.info/tags/银行/"},{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/tags/技术/"}]},{"title":"项目管理之夕会","date":"2017-03-09T13:14:01.000Z","path":"2017/03/09/项目管理之夕会/","text":"近期项目管理过程中使用了 夕会 ，总结如下： 适用场景 项目团队成员较多，各自分工不同，互相间的工作有依赖或者有借鉴之处项目实施沟通交流效率不高，问题解决效率不高项目处于联调等需要紧密沟通，快速响应阶段解决成员工作效率低，质量低，降低项目进度风险 前提 项目整体计划明确，成员的每日工作目标明确项目管理成员持续跟踪项目进展，跟踪问题，收集反馈，更新问题解决进展收集整理夕会要点 要求 参会人员准时参加会议要点描述清晰准确会议时间尽量短 步骤 项目经理（项目跟踪人）讲述跟踪问题的进展，明确问题的责任人和完成计划与会人员可对跟踪问题的提意见/建议，但不展开讨论与会人员补充：待跟进事项，会上明确责任人和反馈时间与会人员向大家通知重要决策和决定 产出物：跟踪事项要点 责任人时间职责交付物 补充夕会 是日计划的一部分，与之对应的有 晨会，晨会主要明确成员的当日工作目标 日计划主要用于项目经理跟踪计划的完成度和完成质量，保证项目的关键任务及时完成 夕会 主要用于日计划的总结，问题的跟踪，明确人员职责","categories":[{"name":"项目管理","slug":"项目管理","permalink":"http://www.sillystone.info/categories/项目管理/"}],"tags":[{"name":"项目管理","slug":"项目管理","permalink":"http://www.sillystone.info/tags/项目管理/"},{"name":"夕会","slug":"夕会","permalink":"http://www.sillystone.info/tags/夕会/"}]},{"title":"吹风","date":"2017-03-05T09:40:03.000Z","path":"2017/03/05/吹风/","text":"屋里空荡荡 心中也空空的 阳光很好 风很大 有蓝天，有白云 背起相机 卸下思念 随手采几张照片 只为填满时间 大图，慎点","categories":[{"name":"摄影","slug":"摄影","permalink":"http://www.sillystone.info/categories/摄影/"}],"tags":[{"name":"摄影","slug":"摄影","permalink":"http://www.sillystone.info/tags/摄影/"}]},{"title":"养鱼记","date":"2017-03-04T01:19:51.000Z","path":"2017/03/04/养鱼记/","text":"我是一只离开水的鱼深夜里 我独自一人 在狭小水缸 思念着你 回忆着 一次又一次 拼尽全力 奇迹般的飞跃 向着美好的希望 只为你 无力的呼吸着 希望越来越远 心越来越静 空气很薄 你在哪儿 孔雀鱼今天发现有一条孔雀鱼总是偷袭其他鱼的尾巴，鱼缸里的大鱼也不放过，被他追的到处跑，看着好滑稽，百度了一下，说是有可能发情期，也可能其他鱼得病了。为了保全其他鱼的尾巴，只能把他隔离了；它游的好快，弄了半天才捞出来 悲剧的是，今天早上发现它不在小鱼缸里，最终发现在离鱼缸挺远的地上，已经变成鱼干了水面上有5/6厘米的高度，不知道怎么跳出来的。 那拼尽全力的一跃，垂死的挣扎，从阳台上坠落，爬了那么远，最终张着嘴却没有呼吸没有结果的结果","categories":[{"name":"随笔","slug":"随笔","permalink":"http://www.sillystone.info/categories/随笔/"}],"tags":[{"name":"诗","slug":"诗","permalink":"http://www.sillystone.info/tags/诗/"}]},{"title":"Sublime Text Tip","date":"2017-03-02T12:31:38.000Z","path":"2017/03/02/Sublime-Tip/","text":"定位 文件定位 CTRL + P 词语定位 CTRL + ; 函数定位 CTRL + R 行号定位 CTRL + G 合并选中的多行代码为一行 CTRL + J 快速折叠文件内所有函数 CTRL + K + 1 编辑 选择一个选中项的下一个匹配项，连续选中相同匹配项 CTRL + D 【MAC：⌘ + D】 选择一个选中项的所有匹配项 ALT + F3 【MAC：CTRL +⌘ + G】 选择与光标关联的开始和结束标签 CTRL + SHIFT + ` 【MAC：⌘+⇧+ K】 按内容层次选择内容，连续使用，选择内容逐渐扩大 CTRL + SHIFT + A 【MAC： CTRL + D】 按括号逐层选择内容，CTRL + SHIFT + M 【MAC：⌘ + ⇧ + Space】 复制行&amp; 复制选中的内容 CTRL + SHIFT + D 【MAC：⌘ + ⇧ + D】 增加 减少缩进 CTRL + [或] 【MAC：⌘ + [ 或 ]】 粘贴并保持缩进 CTRL + SHIFT + V 【MAC：⇧ + ⌘ + V】 用标签包裹行 alt+shift+w 【MAC：CTRL + ⇧ + W】 删除父标签 ctrl+shift+; 【MAC：⌘ + ‘】 计算数学表达式 ctrl+shift+y 【MAC：⌘ + ⇧ + Y】 数字自动增减，10为单位 alt+shift+↑ 或 ↓，ctrl+ ↑ 或 ↓ 【MAC：⇧+OPTION + ↑】 大小写 ctrl+k+u,ctrl+k+l 【MAC： ⌘ + K then U, ⌘ + K then L】 安装package： ctrl + shift + p","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"开发工具","slug":"开发工具","permalink":"http://www.sillystone.info/tags/开发工具/"},{"name":"IDE","slug":"IDE","permalink":"http://www.sillystone.info/tags/IDE/"}]},{"title":"零开始学coding","date":"2017-02-28T14:24:31.000Z","path":"2017/02/28/零开始学coding/","text":"最近学习前端的相关知识，感觉前端知识太多，变化太快，新的技术和组件，甚至是工具变化都很快，感觉学习前端要学好多东西，全栈工程师不容易啊，github上好多国内的高手推荐一个网站：FreeCodeCamp 中文网站： FreeCodeCamp中文网站是从知乎：零基础的前端开发初学者应如何系统地学习看到的，试了一下感觉还不错，交互做的不错。 适合初学者，用来教小朋友也可以。 对于有代码经验的，开始的步骤有些繁琐。 只进行了一部分，不知道后边关于nodejs怎么讲解的","categories":[{"name":"技术","slug":"技术","permalink":"http://www.sillystone.info/categories/技术/"}],"tags":[{"name":"编码，学习","slug":"编码，学习","permalink":"http://www.sillystone.info/tags/编码，学习/"}]},{"title":"难得北京蓝","date":"2017-02-25T13:22:42.000Z","path":"2017/02/25/难得北京蓝/","text":"北京蓝遇上ofo周末活动，回归街拍生活 图片较大,请在wifi环境查看 据说 姚振华 最近很火，来张宝能大厦","categories":[{"name":"摄影","slug":"摄影","permalink":"http://www.sillystone.info/categories/摄影/"}],"tags":[{"name":"摄影","slug":"摄影","permalink":"http://www.sillystone.info/tags/摄影/"},{"name":"街拍","slug":"街拍","permalink":"http://www.sillystone.info/tags/街拍/"}]},{"title":"真","date":"2017-02-25T11:51:16.000Z","path":"2017/02/25/真/","text":"严歌苓 《霜降》 述认识 严歌苓 是从她的作品 《寄居者》 开始，她的小说大多是描述特定的时代，建国前后，人命如草，道德模糊，故事不断。 霜降 是个农村进京的漂亮小姑娘，进了程家的大院，进了一个她奢望但无法进入的阶层。那个年代所有人都被禁锢着，不只是 霜降 这样的小民，也包括大院里的所有人。每个人都无法跑出那个时代，四星 或许最终成功了，出国了。 书的封面讲述了 霜降 的故事 爱，在若有似无的触碰中绝望冬，将临近幸福，如履薄冰。。。。 真 原来爱与过活是两回事，爱一定要过渡到过活才能自然长久地存在下去，过活却不需要爱，过活自身是独立和成熟的，因此它自身能够自然长久地存在。过活不需要你挺累地将目光弄得曲折，将笑摆得那么巧。过活是大米饭，你饿，它结实地填饱你，朴实得让人感动。 真，就是你在眼前，陪伴比爱更直接 引 一个曾经被牢记的人，被人忘记是挺惨的一件事他站在窗前，好似一缕魂看着人间长长一段宁静淡然成了虚伪伦理报复了道德，喜剧报复了悲剧，冤孽报复了冤孽","categories":[{"name":"随笔","slug":"随笔","permalink":"http://www.sillystone.info/categories/随笔/"}],"tags":[{"name":"读后感","slug":"读后感","permalink":"http://www.sillystone.info/tags/读后感/"},{"name":"日记","slug":"日记","permalink":"http://www.sillystone.info/tags/日记/"}]},{"title":"记忆","date":"2017-02-22T16:00:00.000Z","path":"2017/02/23/记忆/","text":"读 村田喜代子 《八口小锅》 锅中 有感 孩子们发掘奶奶记忆的故事一直在农村生活的奶奶虽然身体健康，但记性却很糟糕，记忆总是忽然被触动，又很快淡去故事里的奶奶一直独自居住在村子里，八十岁的她在享受自己的最后时光，对生活似乎没有更多的要求，只是静静的过好每一天，忙碌于生活琐事，间歇努力找寻曾经的回忆。几个孙辈孩子们的到来给她带来很多快乐，当然也触动她久远的回忆，那里有逝去的亲人和他们的子女，那里有清晰的呼唤和模糊的面孔，多年感情在记忆闪现的瞬间澎湃不已，搅动起难以平息的痛。 忙碌的我们基本没有时间思考生命，时光，过去的回忆，直到我们老去 故事里慈祥、安静的奶奶和孩子们的快乐日子让我羡慕不已， 我的这片感情是空白 乡村生活，小菜园子，池塘，庙，大树，青蛙，蚂蚁，好怀念小时候的日子 文中的多美 是个会做菜的孩子，口水流不停啊 摘一段文字 我曾想某一天将奶奶脑子里的情形一探究竟，看看这几十年的记忆是以怎样的形式重叠起来。那时候可能会看到雾气之类的笼罩在这些记忆上。但是，我可以推想，古老的记忆现在就像这些相片一样已经是丢失了专有名词、失去了前后关联、犹如零散胶卷的东西了。 自然就像一个深不见底的大碗。。。。我想","categories":[{"name":"随笔","slug":"随笔","permalink":"http://www.sillystone.info/categories/随笔/"}],"tags":[{"name":"读后感","slug":"读后感","permalink":"http://www.sillystone.info/tags/读后感/"},{"name":"日记","slug":"日记","permalink":"http://www.sillystone.info/tags/日记/"}]}]